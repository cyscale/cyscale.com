{"componentChunkName":"component---src-template-author-page-template-js","path":"/blog/andrei-stefanie/","result":{"data":{"allMarkdownRemark":{"edges":[{"node":{"frontmatter":{"authors":"Andrei Ștefănie","categories":["Cloud Security"],"title":"IPv4 Billing Changes in AWS: Impact on Cloud Costs & Security","seoTitle":"IPv4 Billing Changes in AWS: Impact on Cloud Costs & Security","description":"Explore how AWS's new IPv4 billing changes, effective from February 2024, will influence both your financial and security strategies. Understand the implications for small startups to large enterprises, from cost optimization to IPv6 transition, NAT64, and security enhancements.","seoDescription":"Explore how AWS's new IPv4 billing changes, effective from February 2024, will influence both your financial and security strategies. Understand the implications for small startups to large enterprises, from cost optimization to IPv6 transition, NAT64, and security enhancements.","date":"2023-08-31T09:03:04.258Z","featuredpost":true,"permalink":"aws-ipv4-impact-on-cloud-costs-and-security","featuredimage":{"publicURL":"/static/aa850ad595bb735a8dd4fb05a8339beb/aws-ipv4-article.png","childImageSharp":{"gatsbyImageData":{"layout":"constrained","images":{"fallback":{"src":"/static/aa850ad595bb735a8dd4fb05a8339beb/83d34/aws-ipv4-article.png","srcSet":"/static/aa850ad595bb735a8dd4fb05a8339beb/c4463/aws-ipv4-article.png 205w,\n/static/aa850ad595bb735a8dd4fb05a8339beb/0f295/aws-ipv4-article.png 410w,\n/static/aa850ad595bb735a8dd4fb05a8339beb/83d34/aws-ipv4-article.png 820w","sizes":"(min-width: 820px) 820px, 100vw"},"sources":[{"srcSet":"/static/aa850ad595bb735a8dd4fb05a8339beb/2b0f8/aws-ipv4-article.webp 205w,\n/static/aa850ad595bb735a8dd4fb05a8339beb/36973/aws-ipv4-article.webp 410w,\n/static/aa850ad595bb735a8dd4fb05a8339beb/3ff2d/aws-ipv4-article.webp 820w","type":"image/webp","sizes":"(min-width: 820px) 820px, 100vw"}]},"width":820,"height":428}}}},"rawMarkdownBody":"*Note: post updated January 30, 2024*\n\nBeginning February 1, 2024, Amazon Web Services (AWS) will usher in a significant change by charging for public IPv4 addresses. Whether you are a seasoned cloud engineer or a business decision-maker this shift by one of the world's leading cloud providers will likely have broad implications.\n\nAWS's decision aligns with other major cloud providers that have adopted similar practices, reflecting an industry-wide trend. AWS's involvement is particularly significant given its substantial market presence, and it will likely influence various organizations across different scales and industries.\n\nIn terms of your business and the tech landscape, this change goes beyond additional costs, with the new model affecting network planning, igniting considerations about transitioning to IPv6, and opening opportunities to reassess cloud security strategies.\n\n## The Financial Implications of AWS's New Charging Model for IPv4 Addresses\n\n### Detailed Explanation of Pricing\n\n<br class=\"\" />\n\n<img src=\"/img/aws-ip-cost.png\" alt=\"AWS IP Costs after February 1 2024\" title=\"\" class=\"\" style=\"width:auto;height:auto;\"/>\n\nStarting February 1, 2024, AWS will charge for all public IPv4 addresses at a rate of 0.5 cents per hour. While this figure may initially seem inconsequential, it is vital to understand how swiftly it can accumulate. \n\nFor small businesses or individual projects, the impact might be marginal. Our example considers a small organization with 10 public IP addresses:\n\n*10 IPs x 0.5 cents per hour x 730 hours in a month = $36.5 per month, or $438 per year.*\n\nContrast this with an enterprise operating with 11,000 public IPs: \n\n*11,000 IPs x 0.5 cents per hour x 730 hours in a month = $40,150 per month, or nearly **$500,000 per year**.*\n\n1﻿1,000 IP addresses might sound like an overestimation, but we are seeing organizations with tens of thousands of EC2 instances. Assuming 20-30% of these need public IPs is not out of reach.\n\nThis striking difference highlights the importance of understanding and planning for the financial consequences, particularly for organizations heavily reliant on public IPv4 addresses.\n\n### Budget Considerations for Organizations\n\nAWS's decision to charge for public IPv4 addresses extends beyond a simple financial concern; it prompts businesses to reevaluate their entire approach to IP address utilization. Key considerations include:\n\n* **Reviewing existing usage:** Analyzing current usage patterns is essential for predicting additional costs and identifying opportunities for optimization.\n* **Considering IPv6 migration:** Some may find transitioning to IPv6 a cost-effective alternative, though it must be balanced against compatibility and technical constraints.\n* **Utilizing tools and insights:** Automated security platforms like Cyscale can offer comprehensive insights into your inventory of public IP addresses, helping you understand the big picture and delivering insight that can aid in cost control.\n\nThis policy change serves as a catalyst for reimagining how organizations approach cloud infrastructure financially. The ripple effects will be felt differently across the spectrum, but the core message is clear: understanding, planning, and adapting will be pivotal in navigating this change without unforeseen financial setbacks.\n\nWhether you are a decision-maker concerned about the bottom line or a cloud professional tasked with optimization, these changes necessitate close scrutiny and proactive planning.\n\n## Technical Aspects and Considerations\n\n### Impact on EC2 Instances, RDS, EKS, etc.\n\nThe new charging model impacts a broad array of resources, including EC2 instances, RDS, EKS, load balancers, and more. Managing complex architectures now entails a new layer of complexity, possibly requiring a reassessment of networking strategies and configurations.\n\n### The Complexity of Managing Multiple IP Addresses per Resource\n\nSome resources may have more than one public IPv4 address. Managing these involves not just technical configuration but also financial planning. Understanding how these multiple IPs interact within your infrastructure and contribute to overall costs is essential. \n\n### Discussion on the Feasibility and Challenges of Switching to IPv6\n\nThe move to IPv6 seems logical, but it's not devoid of challenges. Compatibility with services and APIs managed by others might become a hurdle. AWS has made progress, such as enabling communication from EC2 to Lambda functions over IPv6 (note that you cannot reach an EC2 instance from Lambda over IPv6), yet some cases may still hinder a complete switch.\n\n### IPv4 to IPv6 Transition Mechanisms\n\nDuring the transition, Network Address Translation (NAT), specifically NAT64, becomes vital. This mechanism translates IPv6 to IPv4 addresses, bridging communication between newer IPv6 systems and legacy IPv4 systems. Understanding and utilizing NAT64 is crucial for modernizing without losing functionality. Equally important is DNS64 which performs the translation at the DNS level. More specifically, in the case a domain is mapped only to an IPv4 address (i.e., it only has A records), DNS64 translates an A record (specific to IPv4) to an AAAA record (specific to IPv6), thus allowing an IPv6 client to reach an IPv4 server.\n\n### Proficiency with Security Groups and Other Network Considerations\n\nWith all IPv6 addresses being public, mastering security groups and leveraging egress-only internet gateways become even more critical. Properly configuring security rules and comprehending their interaction with different IP versions plays a significant role in upholding security and functionality within your network.\n\nThe new AWS charging model for public IPv4 addresses is more than a financial consideration. It intertwines with a multifaceted web of technical aspects that must be thoughtfully navigated. Embracing the right strategies and tools can pave the way for a seamless transition, preserving both efficiency and budget.\n\n## Security Implications\n\n### Reevaluation of Network Exposure\n\nThe new charges for public IPv4 addresses might encourage organizations to scrutinize network exposure more closely. Reducing the number of public IPs could be both a cost-saving measure and a strategy to enhance security by limiting the **attack surface.**\n\n### Importance of Secure Configuration in IPv6 Transition\n\nTransitioning to IPv6, while potentially cost-effective, demands careful consideration of security configurations. Understanding IPv6 security nuances is vital in safeguarding systems during and after the shift. While NAT64 facilitates communication between IPv6 and IPv4 systems, it also introduces unique security considerations. Proper implementation and continuous vigilance are required to ward off vulnerabilities that might arise during the translation process. In practice, most organizations will leverage the AWS NAT Gateway which already supports NAT64 and DNS64 (through Route 53) so our responsibility includes proper route configuration and keeping DNS resolvers up to date. However, other organizations might choose to deploy their own NAT instance to optimize cost.\n\n### Tools and Platforms for Security Management\n\nAutomated cloud security platforms like Cyscale can play a pivotal role in this secure transition. By providing insights into all public IP addresses, their attachments, and alternative communication paths, they can facilitate a more secure and cost-effective migration. Utilizing specialized security platforms and tools is fundamental to maintaining control over complex cloud environments.\n\nThis new AWS charging model for public IPv4 addresses is more than a technical and financial hurdle; it's an opportunity to rethink and potentially enhance security strategies. By understanding the interplay between IPv4 and IPv6, the role of the transition mechanisms, and the necessity of proper security configurations, organizations can navigate this transition without compromising security.\n\n## The Role of Cloud Security Platforms\n\nAutomated cloud security platforms like Cyscale are more than just a reactive measure to changes like AWS's new charging model for public IPv4 addresses; they represent a proactive approach to modern cloud infrastructure management. By providing tools that cut across cost optimization, security enhancement, migration planning, and collaboration, these holistic solutions enable organizations to thrive in an evolving cloud landscape."}},{"node":{"frontmatter":{"authors":"Andrei Ștefănie","categories":["Cloud Security"],"title":"A Word on Cloud Security","seoTitle":"A Word on Cloud Security","description":"Given all this complexity and the pace at which we are trying to deliver our products, it’s no surprise that breaches still happen. However, we can go fast and build secure systems. It’s not a zero-sum game. You probably have people in your organization that are naturally passionate about security. Give them the responsibility, the training, and the tools. You probably don’t need that many people to do security full-time since the tools are getting increasingly powerful.\n","seoDescription":"Given all this complexity and the pace at which we are trying to deliver our products, it’s no surprise that breaches still happen. However, we can go fast and build secure systems. It’s not a zero-sum game. You probably have people in your organization that are naturally passionate about security. Give them the responsibility, the training, and the tools. You probably don’t need that many people to do security full-time since the tools are getting more and more powerful.","date":"2022-10-25T14:41:49.426Z","featuredpost":true,"permalink":"a-word-on-cloud-security","featuredimage":{"publicURL":"/static/e7d0b79299e2f1d5a1d08d2a9f3629a9/microsoftteams-image-3-.webp","childImageSharp":{"gatsbyImageData":{"layout":"constrained","images":{"fallback":{"src":"/static/e7d0b79299e2f1d5a1d08d2a9f3629a9/888e2/microsoftteams-image-3-.webp","srcSet":"/static/e7d0b79299e2f1d5a1d08d2a9f3629a9/913d0/microsoftteams-image-3-.webp 205w,\n/static/e7d0b79299e2f1d5a1d08d2a9f3629a9/91660/microsoftteams-image-3-.webp 410w,\n/static/e7d0b79299e2f1d5a1d08d2a9f3629a9/888e2/microsoftteams-image-3-.webp 820w","sizes":"(min-width: 820px) 820px, 100vw"},"sources":[]},"width":820,"height":460}}}},"rawMarkdownBody":"## Security is Foundational\n\nWhy care about cloud security? Or about security at all? Let’s zoom out even more and talk about the virtues we, as people, often desire for ourselves and look for in others. These include courage, compassion, generosity, temperance, persistence, and friendliness. As Brian Tracy points out in his excellent book, “No Excuses!: The Power of Self-Discipline”, all these virtues depend on and are guaranteed by one of them: **integrity**. Just like this, our software products have multiple requirements including functional (it must solve specific problems) and non-functional (performance, availability, efficiency, etc.). All these depend on the trust our users have in our systems to keep their data safe; they depend on **security**.\n\nIn theory, the most secure software is the one that doesn’t exist. If you have nothing, there is nothing to worry about. In practice, you will probably want to achieve certain things, to deliver certain features within a reasonable timeframe and budget. Luckily for us, the cloud providers offer a sizeable suite of services that might help us. In the era of data-driven, microservices-based, global-scale systems, we end up using some of them for compute (e.g. EC2, AKS, GKE, Lambda, Cloud Run), storage (S3), business intelligence (Redshift, Power BI), integration (Pub/Sub, Kinesis, API Gateway, SQS, SNS, Azure Service Bus), and so on. AWS even offers a service that lets you make use of their (satellite) ground stations. If you are competing with Elon’s Starlink or you are building a weather forecasting system, you might find it useful\n\n## Great Power, Great Responsibility\n\nNaturally, you must understand how to configure each cloud service you end up using and make them work together. One fundamental concept provided by the cloud is the way you manage access. You have control over both the network (so the service is actually reachable or not) and IAM (whether the clients must have identities and sufficient permissions). On one end of the spectrum, you can leave everything open and hope everyone expects that you have it properly set up. On the other end, you can choose a very restrictive configuration that might slow down your team. This is often known as the tradeoff between security and usability (for your dev team in this case). Over the past few years, the industry started shifting more toward security introducing concepts such as zero trust security and SSO (please make use of it whenever possible; also, it’s a shame that some companies still provide SSO only with their enterprise plans).\n\nClassically, you had a number of VMs (on-premises, VPS), installed and configured UFW, set up SSH access, and sent the SSH keys to your colleagues. Now, you can configure public and **private** (sub)networks, connect your cloud networks to your on-premises networks, deploy load balancers, define firewall rules without ever SSH-ing into the VMs, and assign identities and permissions to the VMs (and any other compute service). If previously your team didn’t interact too much with the infrastructure side, **now the infrastructure is part of the application**.\n\n## Network Access\n\nStarting with configuring the network access, here you have multiple options to control it. Perhaps the simplest concept is making the resources public or private. Your VM might have a vulnerable OS version, but if nothing can reach it and it cannot reach the internet, the risk is considerably lower. Not only can you run most compute and database services in a private network, but you can also configure fully managed services to be accessible through the provider's internal network (without traversing the internet) by leveraging services such as [AWS PrivateLink](https://aws.amazon.com/privatelink/) and [Azure Private Link](https://learn.microsoft.com/en-us/azure/private-link/private-link-overview).\n\nFurthermore, you can (have to) configure firewalls. Here the services vary a bit. Google Cloud calls them [VPC firewall rules](https://cloud.google.com/vpc/docs/firewalls). You define them at the VPC level and can optionally choose to which instances they should apply based on tags (they are not equivalent to AWS/Azure tags. In Google Cloud, the equivalent would be the resource labels) or service accounts (almost every resource can have an identity, which in Google Cloud, is given by service accounts). In AWS, you work with security groups that are assigned to virtual machines. Besides allowing traffic (by default everything is denied) from certain IP addresses or CIDRs, you can also choose based on the security group of the source (please make use of this). You can also achieve this in Azure, but you have to combine two types of resources: network security groups (these are the actual firewalls) and application security groups (you can associate these with the network tags from Google Cloud). In fact, here are the sources you can configure for each provider:\n\n* AWS: CIDRs, IP addresses, security groups, prefix lists (if you want to allow traffic coming only from certain AWS services)\n* Azure: CIDRs, IP addresses, application security groups, service tags (e.g. \\`AzureLoadBalancer\\`, \\`Internet\\`)\n* Google Cloud: CIDRs, IP addresses, network tags (defined on each instance), service accounts\n\nOf course, this is not all. You also have network access control lists (NACLs), firewalls specific to each service (especially managed databases from Azure), NAT gateways, VPC peering, VPNs, virtual appliances, traffic splitting, firewalls that you install on the VMs, and a lot more. All these bring considerable complexity and make it extra difficult to configure the optimal **effective network access**.\n\nAdditionally, while there are tools that check the traffic in real-time, these are often dealing with network paths that you defined and are aware of. There might be paths you did not intend to leave open that an attacker might make us for **lateral movement**.\n\n## IAM Access\n\nNot only can every member of your organization have a user (or multiple) with access to certain parts of your infrastructure, but so does almost every compute resource. Just like you provide John with permission to read data from a bucket, so you assign a role to a VM achieving the same access. The difference is that anything that runs on the VM and everyone who can access it (i.e. SSH into it) now can do everything the VM can do.\n\nFundamentally, this mechanism of assigning roles to resources is excellent. You have clear visibility of the principals (users, resources) that can access a given service, you can grant or revoke access at any moment, and you have a meaningful trail of access logs. The alternative is often based on keys/secrets and is almost impossible to trace properly because anyone that has the key can access the resource (it’s easy to lose track of who has the key) (secret scanners help a bit here but are limited to the resources they run on).\n\nWe just have to understand its implications. If attackers manage to break into a VM and if that VM has high (excessive?) permissions, the attackers will not only be able to access our data, but they will also be able to spin up additional instances (crypto mining?) and delete existing resources (service disruption and data loss).\n\nAgain, here it’s important to understand the effective permissions each resource has. There are multiple factors that contribute to the effective permissions such as the role assigned to the resource, permissions inherited from group memberships or from assignments at a higher level (the VM might be an owner at the subscription level in Azure thus having access to everything in that subscription), and policies configured on the target resources such as [resource policies in AWS](https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_identity-vs-resource.html) and [access policies for Azure Key Vaults](https://learn.microsoft.com/en-us/azure/key-vault/general/security-features).\n\nAnother important aspect about effective permissions in the context of data access is encryption, especially encryption with customer-managed keys. For example, if you store data on S3 and encrypt it with a key managed by AWS KMS, you must have access to both the S3 bucket/object and the encryption key from KMS. Managing encryption keys deserves an article on its own.\n\n## The Human Factor\n\nEven if you have pristine infrastructure security, you still must provide access to someone (at least two admins so one doesn’t lock himself out by mistake - aka admin redundancy) to maintain the infrastructure and manage access for everyone else. This is an example of a highly privileged user. This in itself is not a major risk. However, if highly privileged users don’t have strong authentication mechanisms in place (MFA, preferably with strong factors), they can become liabilities. We are all subject to social engineering and phishing attacks.\n\nWhen you find out that an attacker managed to gain access to a highly privileged user, after revoking the sessions and rotating the credentials, you will want to find out the impact. For this, you need to know the systems to which the user has access. This can be quite challenging since the permissions are often spread across multiple systems.\n\nFor example, if you have Okta as the identity provider and AWS for the cloud infrastructure, you have to check both systems and link the results in order to determine what a person actually has access to. In Okta you just provide access to an application, but in AWS that application is often an AWS Organization with multiple accounts.\n\n## Ending Notes\n\nGiven all this complexity and the pace at which we are trying to deliver our products, it’s no surprise that breaches still happen. However, we can go fast and build secure systems. It’s not a zero-sum game. You probably have people in your organization that are naturally passionate about security. Give them the responsibility, the training, and the tools. You probably don’t need that many people to do security full-time since the tools are getting increasingly powerful.\n"}},{"node":{"frontmatter":{"authors":"Andrei Ștefănie","categories":["Product"],"title":"Integrating NATS Into the Cyscale Platform","seoTitle":"Integrating NATS Into the Cyscale Platform","description":"Some concepts and techniques we leveraged to switch to a cloud-native message broker.","seoDescription":"Discover how Cyscale has enhanced cloud security integration by utilizing NATS on Kubernetes for efficient, scalable message-based communication. Read about the journey, benefits, and technical implementation on our blog.","date":"2021-12-22T10:11:16.968Z","featuredpost":true,"permalink":"integrating-nats-into-the-cyscale-platform","featuredimage":{"publicURL":"/static/cb4216e63701672e106156941ccdecaf/nats-horizontal-color.webp","childImageSharp":{"gatsbyImageData":{"layout":"constrained","images":{"fallback":{"src":"/static/cb4216e63701672e106156941ccdecaf/02dc4/nats-horizontal-color.webp","srcSet":"/static/cb4216e63701672e106156941ccdecaf/913d0/nats-horizontal-color.webp 205w,\n/static/cb4216e63701672e106156941ccdecaf/85995/nats-horizontal-color.webp 410w,\n/static/cb4216e63701672e106156941ccdecaf/02dc4/nats-horizontal-color.webp 820w,\n/static/cb4216e63701672e106156941ccdecaf/62a3a/nats-horizontal-color.webp 1640w","sizes":"(min-width: 820px) 820px, 100vw"},"sources":[]},"width":820,"height":461}}}},"rawMarkdownBody":"\n<!--StartFragment-->\n\n## Backstory\n\nCyscale helps you secure your cloud infrastructure. To achieve this, the platform must be able to read (sync) the cloud resources, perform assessments against a set of controls (security and architecture guidelines and best practices), send notifications, generate reports, perform scheduled tasks, and so on. Given the highly distributed and segregated nature of the platform, we chose a microservices architecture on top of Kubernetes. Also, by design, these processes are mostly asynchronous, happening in the background as a result of a certain event or trigger.\n\nBesides direct HTTP communication (REST mostly for the account/user management and GraphQL for everything cloud-related), we make heavy use of message-based communication.\n\nUp until recently, Redis served as our backbone for sending messages. We used Redis Lists to simulate queues (e.g., for sending emails) and Redis Pub/Sub for, well, implementing the publish-subscribe pattern (e.g., for triggering the synchronization of the cloud resources). We knew since the beginning that Redis will not serve as the messaging middleware forever, but we started with it since it was already there for caching and tasks (through [Go Celery](https://github.com/gocelery/gocelery)).\n\nDue to a mix of accumulated technical debt and a desire for simplicity, we decided to invest in integrating a purpose-built technology for handling messages. After a good amount of research covering topics such as operational simplicity, community, and documentation, we decided to go with [NATS](https://nats.io/). As a side note, we continue to love Redis and there are plenty of well-established companies using it as their messaging middleware with great success.\n\nThe rest of the article will cover the main steps we took to integrate NATS into our platform such as understanding the NATS ecosystem, deploying the relevant tools on our Kubernetes cluster using Helm - this will form the main part since this is where we faced the most challenges, and, of course, sending and consuming messages.\n\n<!--EndFragment-->\n\n<!--StartFragment-->\n\n**The NATS Ecosystem**\n\n![The NATS ecosystem encompassing the core NATS server, JetStream, the NATS clients and CLI, and the NATS resources for Kubernetes](/img/cyscale-nats.webp 'The NATS ecosystem')\n\n### Core NATS\n\nAt its core, **the NATS server** is a **publish/subscribe** message broker. It offers **at most once delivery** and works based on **subjects**. These can have a hierarchical structure such as `sync.aws` and `sync.gcp`. Services concerned with messages related to syncing operations for AWS will only subscribe to `sync.aws` while other services might listen for all sync-related messages on `sync.*` (which covers both subjects) or even `sync.>` (which will also cover `sync.aws.ec2` - a separate subject).\n\nAnother feature that provides us with great value is called **queue groups**. This helps us horizontally scale our consumers while making sure that only one instance of a service receives a certain message. If you have experience with Kafka, it resembles consumer groups. What’s nice about queue groups is that they are automatically created when consumers subscribe to a subject and provide the queue group parameter (a simple string that, just like subjects, can have a hierarchical structure). For example, we use the name of the service (e.g. notifier) as the queue group.\n\nWhile the core functionality is great and simple, plain old pub/sub with at most once delivery will not cover all use cases. Subscribers might be under heavy load or even down, there might be a network partition or we might even want to keep messages and follow an approach based on event sourcing (you can find more examples in the [NATS docs](https://docs.nats.io/using-nats/developer/develop_jetstream)). In other words, as soon as we need **temporal decoupling** between publishers and subscribers, we need **persistence**, which for NATS is provided by JetStream.\n\n### JetStream\n\nJetStream adds the concept of **streams** on top of the core NATS subjects. Basically, if you want your messages to be persisted, you can enable JetStream on the NATS server and create a stream that will actually store the messages sent to a subject (or multiple subjects - this is mostly to optimize resource utilization; for example, we have one stream called sync that stores all messages sent to any sync subject - i.e. sync.>).\n\nIf your system takes full advantage of JetStream, at most once delivery transforms into **at least once** and even **exactly once** by leveraging [message deduplication](https://docs.nats.io/using-nats/developer/develop_jetstream/model_deep_dive#message-deduplication) (NATS will discard a message if another message with the same publisher-provided ID exists in the stream over a window of a certain time - 2 minutes by default).\n\nOne confusion we had at the beginning was whether we actually had to do anything to take advantage of JetStream besides enabling it. Again, bringing the simplicity up front, publishers will not require any modification unless you are looking for exactly-once delivery (you will have to add the message ID). They still send messages on a certain subject and, behind the scenes, JetStream will persist them in the configured stream.\n\nOn the other hand, **you do have to create the actual streams** (we will talk about this below) and **adjust the subscribers to use the JetStream API** (part of the client NATS library). Note that you can still use the core NATS API, but your subscribers will not receive messages sent before they started listening (even though they are stored in the stream). The reason behind this is that JetStream actually creates consumers that handle the delivery of messages for each subscriber. You will have to manually create the consumer when using the CLI, but the client libraries will handle this automatically when subscribing to a subject through the JetStream API.\n\nHaving the messages persisted also enables us to take different approaches based on what we want to achieve. We might still follow a pub/sub approach for certain subscribers (these are known as **push consumers**) (e.g. we use this approach for generating user-requested reports) or we might want to have more control over how messages are retrieved in which case we will use a **pull consumer**. This enables us to batch messages (e.g. we do this for sending notifications). Here is how we handle the messages in our notifier service:\n\n```typescript\nexport type MessageHandler = (data: NotificationDto[]) => Promise<void>;\n\nexport const handleNotificationMessages = async (handler: MessageHandler) => {\n    try {\n        const nc = await getNatsClient();\n\n        if (!process.env.NOTIFICATIONS_SUBJECT) {\n            throw Error('NOTIFICATIONS_SUBJECT not set');\n        }\n\n        const jc = JSONCodec<NotificationDto>();\n\n        const psub = await nc.jetstream().pullSubscribe(process.env.NOTIFICATIONS_SUBJECT, {\n            queue: 'notifier',\n            config: { durable_name: 'notifier' }\n        });\n\n        const done = (async () => {\n            let notifications: NotificationDto[] = [];\n            for await (const m of psub) {\n                try {\n                    notifications.push(jc.decode(m.data));\n                    m.ack(); // Wait to gather all messages from the current batch\n                    if (m.info.pending === 0) {\n                        await handler(notifications);\n                        logger.info(`Processing ${notifications.length} messages`);\n                        notifications = [];\n                    }\n                } catch (error) {\n                    logger.error(error);\n                }\n            }\n        })();\n\n        setInterval(() => {\n            psub.pull({ batch: 30, expires: 1000 });\n        }, 1000 * 30);\n\n        logger.info(`Listening for messages on ${process.env.NOTIFICATIONS_SUBJECT}`);\n\n        await done;\n        await psub.destroy();\n    } catch (e) {\n        logger.error(`Failed to initiate message listening ${e}`);\n    }\n};\n```\n\n(yes, most of our services are actually written in Go, hence the naming of some variables)\n\nAnother dilemma we faced was regarding **stream creation**. Who/what is responsible for creating the streams? One option is using the client libraries which expose a method to **idempotently** create streams. While this can work just fine, we didn’t want our services to bother with the technical details of NATS. Also, streams felt more like being part of the infrastructure than part of the actual services. So we continued our research and found [NACK](https://github.com/nats-io/nack) which we cover below.\n\nAs a side note regarding persistence/streaming with NATS, the precursor of JetStream is called STAN, which is now deprecated. We are mentioning this because there are still plenty of tutorials that focus on STAN, but JetStream is the way forward.\n\n<!--EndFragment-->\n\n<!--StartFragment-->\n\n## NATS on Kubernetes with Helm\n\nBeing part of the CNCF, we can expect NATS to have first-class support for Kubernetes. And it does.\n\nSince the entire Cyscale platform is specified as a Helm chart we just needed to add the [NATS](https://github.com/nats-io/k8s/tree/main/helm/charts/nats) subchart as a dependency and configure the values. You can check the [values file](https://github.com/nats-io/k8s/blob/main/helm/charts/nats/values.yaml) from the chart repo for reference. One small detail that cost us a few hours was how we were specifying the values for NATS. If you look at [the documentation](https://docs.nats.io/running-a-nats-service/introduction/running/nats-kubernetes/helm-charts#jetstream), you will notice the `nats` object. However, since we are deploying NATS as a subchart, we will need an additional parent `nats` object to instruct Helm to pass the values down to the nats subchart. Here are our values for NATS on the dev cluster:\n\n```yaml\nnats:\n  nats:\n    image: nats:alpine\n    resources:\n      requests:\n        cpu: 100m\n        memory: 100Mi\n      limits:\n        cpu: 200m\n        memory: 200Mi\n    jetstream:\n      enabled: true\n      memStorage:\n        enabled: true\n        size: 80Mi\n      fileStorage:\n        enabled: true\n        size: 1Gi\n        storageDirectory: /data/\n        storageClassName: default\n```\n\n(notice the two `nats`)\n\nBesides the actual NATS server (which is a container running in the NATS pod along with the monitoring and config reloader containers), we also have a **NATS Box** pod (comes with the NATS Helm chart) that helps us with testing and administrative tasks - basically its a **preconfigured NATS CLI**. We access it using the command `kubectl exec -it <nats-box-container> -- /bin/sh -l`. The other alternative would have been to install the NATS CLI on our machines and port forward the NATS server from the cluster.\n\n### Creating the Streams with NACK\n\nBesides NATS, we also added the [NACK subchart](https://github.com/nats-io/k8s/tree/main/helm/charts/nack) which requires the NACK **CRDs** (install using `kubectl apply -f <https://raw.githubusercontent.com/nats-io/nack/v0.6.0/deploy/crds.yml>`). That’s because it enables us to treat JetStream streams as Kubernetes resources deployed as part of the rest of the platform.\n\nInstead of having our services handle the stream creation or manually creating them from the NATS box, we specify them declaratively as follows (`templates/nats-streams.yaml`):\n\n```yaml\n# See https://github.com/nats-io/nack/blob/main/deploy/crds.yml for more properties\n{{- range .Values.nack.streams }}\napiVersion: jetstream.nats.io/v1beta2\nkind: Stream\nmetadata:\n  name: {{ .name | quote }}\nspec:\n  name: {{ .name }}\n  subjects: {{ .subjects }}\n  storage: {{ .storage | quote | default \"file\" }}\n  retention: {{ .retention | quote | default \"limits\" }}\n---\n{{- end }}\n```\n\nWe also declare the streams as a list in the values file (notice the `range`).\n\nOnce these are deployed, you can inspect the streams just like any other k8s resource using `kubectl get streams`. One issue we faced was that the streams were not actually created in JetStream (`nats stream ls` from the nats box) even though the k8s resources existed. We simply manually deleted them from the cluster (`kubectl delete streams.jetstream.nats.io --all`) and re-deployed the helm chart.\n\nThere is another alternative we considered - the [jetstream Terraform provider](https://registry.terraform.io/providers/nats-io/jetstream/latest/docs). While we do use Terraform to declare our infrastructure on top of which the Kubernetes cluster is running, we chose NACK because it fit our abstraction layers best and because the terraform provider, running locally or in our pipelines, has to somehow reach the NATS server. In our case, the NATS server is not exposed outside of the cluster (again, port-forwarding is an option).\n\n### Ending Notes\n\nWhile there are more subjects to cover such as [authentication](https://docs.nats.io/running-a-nats-service/configuration/securing_nats/auth_intro) and [authorization](https://docs.nats.io/running-a-nats-service/configuration/securing_nats/authorization), clustering (and [super-clusters](https://docs.nats.io/running-a-nats-service/configuration/gateways)), and multi-tenancy using [accounts](https://docs.nats.io/running-a-nats-service/configuration/securing_nats/accounts), we hope this article helps you better understand how NATS works and how to deploy it. This is our way of giving back to a growing community and expressing our appreciation for getting to work with such great technologies.\n\n<!--EndFragment-->\n"}},{"node":{"frontmatter":{"authors":"Andrei Ștefănie","categories":["Compliance"],"title":"Bridging the Gap Between ISO 27001 and Cloud-Native Systems","seoTitle":null,"description":"Being compliant is a huge selling point and many well-established customers will even require this from their service providers. One missing product feature will probably not make or break the deal, but missing compliance will.","seoDescription":null,"date":"2021-08-31T12:54:05.925Z","featuredpost":true,"permalink":"implementing-iso-27001-for-cloud-native-systems","featuredimage":{"publicURL":"/static/370d42a6af43f75d9ec833998dda8c2c/romain-dancre-doplsdelx7e-unsplash.webp","childImageSharp":{"gatsbyImageData":{"layout":"constrained","images":{"fallback":{"src":"/static/370d42a6af43f75d9ec833998dda8c2c/f0c3d/romain-dancre-doplsdelx7e-unsplash.webp","srcSet":"/static/370d42a6af43f75d9ec833998dda8c2c/e58e4/romain-dancre-doplsdelx7e-unsplash.webp 205w,\n/static/370d42a6af43f75d9ec833998dda8c2c/1cfb4/romain-dancre-doplsdelx7e-unsplash.webp 410w,\n/static/370d42a6af43f75d9ec833998dda8c2c/f0c3d/romain-dancre-doplsdelx7e-unsplash.webp 820w,\n/static/370d42a6af43f75d9ec833998dda8c2c/c9029/romain-dancre-doplsdelx7e-unsplash.webp 1640w","sizes":"(min-width: 820px) 820px, 100vw"},"sources":[]},"width":820,"height":615}}}},"rawMarkdownBody":"\n<!--StartFragment-->\n\n<sub><sup>Photo by [Romain Dancre](https://unsplash.com/@romaindancre?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText) on [Unsplash](https://unsplash.com/?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)</sup></sub>\n\n<!--EndFragment-->\n\n<!--StartFragment-->\n\nAnd no, unfortunately, ISO/IEC 270**17** - which focuses on cloud services - doesn't fill in the gap. Or at least not entirely.\n\nThe trend over the past few years has been clear: shift left. The idea is to identify and prevent issues (not only security-related) as soon as possible. This implies more collaboration during the day-to-day development and even mixed teams that handle all the aspects of the product (there is no limit to how many words we can fit between “Dev” and “Ops”).\n\nWhile some might be inclined to believe organizations are doing this just to cut down costs (of course this is one of the reasons and it should be), the reality is that the team that builds the product also knows the product best. A virtual machine that can theoretically be accessed on port 22 (SSH) from any IP might mean a security hazard to someone that sees the system for the first time. On the other hand, the team will probably have more context and will be able to provide a more accurate risk assessment - maybe the VM is not actually accessible over the internet because it's deployed in a private subnet. Context matters.\n\nAnd the truth is that most teams will happily take on more responsibility and/or collaborate closely with their colleagues. Actually, the best teams want and demand this. They want to contribute to the architecture of the product, be involved in the risk assessments, and help with sales. They will gladly do this as long as the **purpose is clear** and **the job involves actually building something** (this is what software engineers refer to as fun).\n\nOn the other hand, pursuing something like compliance with ISO 27001 which involves reading and writing a lot, might be met with some resistance. While some might get “fooled” when being told they will **build** an Information Security Management **System** (**ISMS** from now on), most people will stop listening when they hear compliance (are you still reading this?). But it doesn't have to be like this.\n\nBeing compliant is a huge selling point and many well-established customers will even require this from their service providers. One missing product feature will probably not make or break the deal, but missing compliance will.\n\n## The Standard\n\nThe ISO/IEC 27001:2013 standard is targeted towards information security. It guides us in the process of building an ISMS that takes a holistic approach (i.e. look at the facts from multiple perspectives) to achieve better **confidentiality, integrity, and availability of our assets (i.e. reduce risks)**.\n\nSince it can be applied in a wide range of industries and companies, ISO 27001 provides general recommendations and approaches. It doesn't tell us how to do anything and there is no recipe for success. **Looking through a simplified view, ISO 27001 guides us through our journey of discovering our assets, identifying risks, and devising strategies for eliminating, mitigating, and remediating the relevant risks.** These can go from ensuring we use encryption to having a designated responsible for contacting authorities when bad things happen (and they will happen).\n\nSo basically it helps us document how we protect our assets. Now, these assets can be anything from physical devices (e.g. laptops) to data stored in an S3 bucket. The standard covers all kinds of assets and situations such as a new employee joining the company (how are they assigned access? have we performed the relevant background checks?) or segregating our networks based on users and services.\n\nIn fact, ISO 27001 provides 114 controls structured in 14 sections (called clauses), each covering a specific area. For example, clause A.13 refers to communications security and has two objectives:\n\n-   Network security management\n-   Information transfer\n\nBoth objectives contain several so-called **controls** that aim to help us **reduce the risk** when applicable. One of the controls, A.13.2.3, refers to _Electronic messaging_ and says the following:\n\n_Information involved in electronic messaging shall be appropriately protected._\n\nThe control tells us what should be achieved, but doesn't tell us how. This is where the compliance team and the development team (usually represented by the architect) start gathering information. Their duty is to research the options and possibilities they have and to find out how the system/application handles it currently.\n\n### Translating ISO controls to the cloud\n\nAssuming they are building their software on top of a hyperscaler (AWS, Azure, Google Cloud), they might start by enforcing HTTPS traffic through security groups/firewalls and leveraging private subnets. This should be good enough for virtual machines, but their applications might use more services such as object storage (S3, Azure Blob Storage, GCS), load balancers, managed databases (RDS, Cloud SQL, the Azure SQL suite of services), integration services such as queueing and pub/sub systems, and the list goes on and on (and probably got longer while you were reading this article).\n\nAfter 6 days of research, they might have identified the following options, let's say on AWS:\n\n1. Since they have been using AWS for the past 9 years, they have to make sure all their EC2 instances are migrated and deployed in a VPC, as opposed to EC2-Classic.\n2. The security groups attached to the load balancers should allow traffic only on port 443.\n3. The load balancers should have TLS listeners set up and accept only safe cipher suites - e.g. it shouldn't accept TLS 1.0 or older SSL protocols that are considered unsafe.\n4. The security groups attached to virtual machines running the application should only accept traffic from the load balancer.\n5. Self-hosted databases such as Elasticsearch should only accept traffic from the EC2 instances running the application.\n6. RDS instances should not be publicly accessible and should only accept traffic from the same EC2 instances.\n7. S3 buckets should have a bucket policy that enforces HTTPS.\n8. S3 buckets should not allow public access through the bucket policy or the ACL (some buckets are pretty old so they heavily rely on ACLs).\n9. And finally, since they leverage AMIs, they would like to ensure that these are private.\n\nCovering all these should provide a great starting point to ensure electronic messaging security. The compliance team adds them to the policy (remember, the output of ISO 27001 is an ISMS which is a set of policies), the development team configures the infrastructure, and the target is achieved. Or is it?\n\n### If it hurts, do it more often\n\n**Adopting a standard means setting a new bar**. It means that all changes from now on must respect the new rules. Of course, the development team is unlikely to print the security policies and attach them to their wall so the product owner creates tasks with checklists as part of their stories. Everything is great now. Every time they add a new cloud resource all they have to do is to go through a 235 items checklist. And also they have to add the resource to their asset inventory.\n\nBeing smart engineers, the development team decides to use AWS Config and implement custom rules to perform these checks whenever a resource is changed. They plan to implement this over the next 4 months and try to keep the compliance team in the loop because they don't want to deal with a stack of Word documents (even though they represent the heart and soul of the compliance team).\n\nNow, if your organization is at this point, you adopted the DevOps culture at this level and you have all these resources, you will probably be fine. Of course, you will have to:\n\n1. practically spam all your employees with presentations about the policies\n2. train and retrain everyone involved including every new employee\n3. wonder whether anyone has actually ever read the policies\n4. wonder whether your research was comprehensive enough\n5. oppose adopting any new cloud provider because it would mean implementing and mapping all the controls again and again\n\n**However, if this sounds like a lot of work to do and/or you want to take your [ISO 27001 Compliance & Cloud Security](https://cyscale.com/use-cases/iso-27001-compliance/) to the next level by giving power to your policies and ultimately to your people, check out [cyscale.com](https://app.cyscale.com/ 'https://app.cyscale.com/').**\n\n<!--EndFragment-->\n"}}]}},"pageContext":{"limit":9,"skip":0,"numPages":1,"currentPage":1,"authors":"Andrei Ștefănie","seoTitle":"Andrei Ștefănie - Cloud Security Insights from a Product Engineer - Cyscale","seoDescription":"Join Andrei, Product Engineer at Cyscale, as he delves into scalable cloud solutions and offers key insights into engineering.","authorSlug":"andrei-stefanie"}},"staticQueryHashes":["220583031","3722074465","4068795820","4109069157","81406208","981947644"],"slicesMap":{}}