{"componentChunkName":"component---src-template-blog-template-js","path":"/blog/pci-dss-version-4/","result":{"pageContext":{"alldata":{"frontmatter":{"authors":"Sabrina Lupșan","categories":["Compliance"],"title":"Compliance Countdown: Navigating the Transition to PCI DSS Version 4","seoTitle":"Compliance Countdown: Navigating the Transition to PCI DSS Version 4","description":"The new PCI DSS version has been released, and the old PCI DSS standard will be deprecated as of the 31st of March, 2024. In this article, we will discuss all the deadlines by which you have to become compliant, details about the new requirements, and everything else you need to know about PCI DSS version 4.\n","seoDescription":"The new PCI DSS version has been released, and the old PCI DSS standard will be deprecated as of the 31st of March, 2024. In this article, we will discuss all the deadlines by which you have to become compliant, details about the new requirements, and everything else you need to know about PCI DSS version 4.","date":"2023-07-20T09:21:54.388Z","featuredpost":true,"permalink":"pci-dss-version-4","featuredimage":{"publicURL":"/static/176523e56bb9c620ee9eca495d6e187c/45_blog-pci-v4.webp","childImageSharp":{"gatsbyImageData":{"layout":"constrained","images":{"fallback":{"src":"/static/176523e56bb9c620ee9eca495d6e187c/888e2/45_blog-pci-v4.webp","srcSet":"/static/176523e56bb9c620ee9eca495d6e187c/913d0/45_blog-pci-v4.webp 205w,\n/static/176523e56bb9c620ee9eca495d6e187c/91660/45_blog-pci-v4.webp 410w,\n/static/176523e56bb9c620ee9eca495d6e187c/888e2/45_blog-pci-v4.webp 820w","sizes":"(min-width: 820px) 820px, 100vw"},"sources":[]},"width":820,"height":460}}},"tableOfContents":true},"rawMarkdownBody":"The new [PCI DSS](https://cyscale.com/blog/pci-dss-compliance-in-cloud/) version has been released, and the old PCI DSS standard will be deprecated as of the 31st of March, 2024. In this article, we will discuss all the deadlines by which you have to become compliant, details about the new requirements, and everything else you need to know about PCI DSS version 4.\n\n## Timeline and deadlines\n\nPCI DSS version 4 appeared at the end of March 2022 and its requirements will become mandatory in two phases: the first one is March of 2024, and the final one is March of 2025.\n\nThe new version brings considerable changes, but most of the new requirements will not be mandatory until 2025. However, companies that manage to fulfill them before 2025 can audit them.\n\nThe old version, PCI DSS 3.2.0., will remain active until the 31st of March, 2024, to allow companies enough time to recertify with the new version.\n\n## Changes in PCI DSS version 4, explained\n\nNew requirements have been added to all of the categories to better reflect the current cybersecurity landscape. Some examples of areas covered by the new requirements include the authentication process, logging and monitoring, vulnerability scanning, and others.\n\nMost of the sections have been revised or contain new requirements.\n\nPCI DSS is structured into twelve requirements, and each contains its own sections. The requirements are:\n\n### 1. Install and maintain network security controls\n\nThis requirement now specifies “network security controls” instead of “firewalls”, to cover more network security technologies. All of the sections in this category have been changed to the following:\n\n* 1.1 Processes and mechanisms for installing and maintaining network security controls are defined and understood.\n* 1.2 Network security controls (NSCs) are configured and maintained.\n* 1.3 Network access to and from the cardholder data environment is restricted.\n* 1.4 Network connections between trusted and untrusted networks are controlled.\n* 1.5 Risks to the CDE from computing devices that are able to connect to both untrusted networks and the CDE are mitigated.\n\n### 2. Apply Secure Configurations to All System Components\n\nThis requirement was initially named “Do not use vendor-supplied defaults for system passwords and other security parameters.”. Now, it refers to all of the company’s configurations, and not only to using vendor-supplied defaults. The sections in this category are the following:\n\n* 2.1 Processes and mechanisms for applying secure configurations to all system components are defined and understood.\n* 2.2 System components are configured and managed securely.\n* 2.3 Wireless environments are configured and managed securely.\n\n### 3. Protect Stored Account Data\n\nWith the third requirement, the focus is switched to account data, compared to the old version, where “cardholder data\" was mentioned instead. New sections are added in relation to PAN (Primary Account Number) and SAD (Sensitive Authentication Data) which cover the storage, encryption and hashing of this data.\n\n* 3.1 Processes and mechanisms for protecting stored account data are defined and understood.\n* 3.2 Storage of account data is kept to a minimum.\n* 3.3 Sensitive authentication data (SAD) is not stored after authorization.\n* 3.4 Access to displays of full PAN and ability to copy cardholder data are restricted.\n* 3.5 Primary account number (PAN) is secured wherever it is stored.\n* 3.6 Cryptographic keys used to protect stored account data are secured.\n* 3.7 Where cryptography is used to protect stored account data, key management processes and procedures covering all aspects of the key lifecycle are defined and implemented.\n\n### 4. Protect Cardholder Data with Strong Cryptography During Transmission Over Open, Public Networks\n\n“Strong cryptography” is now part of the 4th requirement name, to highlight the importance of using appropriate algorithms. The new sections introduce rules regarding the storage and usage of keys and certificates. \n\n* 4.1 Processes and mechanisms for protecting cardholder data with strong cryptography during transmission over open, public networks are defined and documented.  \n* 4.2 PAN is protected with strong cryptography during transmission. \n\n### 5. Protect All Systems and Networks from Malicious Software \n\nThis requirement has been renamed, replacing “anti-virus” with “malicious” software to cover a broader range of technologies. Moreover, sections in this category now regulate the frequency of scans and checks, as well as other aspects.\n\n* 5.1 Processes and mechanisms for protecting all systems and networks from malicious software are defined and understood.\n* 5.2 Malicious software (malware) is prevented, or detected and addressed.\n* 5.3 Anti-malware mechanisms and processes are active, maintained, and monitored.\n* 5.4 Anti-phishing mechanisms protect users against phishing attacks.\n\n### 6. Develop and Maintain Secure Systems and Software\n\nIn this requirement, one of the new sections added (6.4) refers to automating the detection and prevention of attacks over public-facing web apps. Sections 6.1 and 6.3 also contain new requirements.\n\n* 6.1 Processes and mechanisms for developing and maintaining secure systems and software are defined and understood.\n* 6.2 Bespoke and custom software are developed securely.\n* 6.3 Security vulnerabilities are identified and addressed.\n* 6.4 Public-facing web applications are protected against attacks.\n* 6.5 Changes to all system components are managed securely.\n\n### 7. Restrict Access to System Components and Cardholder Data by Business Need to Know\n\nThis requirement includes new rules regarding the review of user accounts and their privileges, as well as the management and privileges of applications and system accounts.\n\n* 7.1 Processes and mechanisms for restricting access to system components and cardholder data by business need to know are defined and understood.\n* 7.2 Access to system components and data is appropriately defined and assigned.\n* 7.3 Access to system components and data is managed via an access control system(s).\n\n### 8. Identify Users and Authenticate Access to System Components \n\nMany new sections regarding MFA, credentials and the entire authentication process have been added in this requirement. Two examples of changes that companies compliant with PCI DSS have to implement now are: the minimum length of a user account password has increased from 7 to 12 characters, and companies are not allowed to store passwords/passphrases in scripts or files.\n\n* 8.1 Processes and mechanisms for identifying users and authenticating access to system components are defined and understood.\n* 8.2 User identification and related accounts for users and administrators are strictly managed throughout an account’s lifecycle.\n* 8.3 Strong authentication for users and administrators is established and managed.\n* 8.4 Multi-factor authentication (MFA) is implemented to secure access into the CDE.\n* 8.5 Multi-factor authentication (MFA) systems are configured to prevent misuse.\n* 8.6 Use of application and system accounts and associated authentication factors is strictly managed.\n\n### 9. Restrict Physical Access to Cardholder Data \n\nThis requirement does not have many changes. Some sections that refer to roles and responsibilities, as well as the frequency of device inspections have been added.\n\n* 9.1 Processes and mechanisms for restricting physical access to cardholder data are defined and understood.\n* 9.2 Physical access controls manage entry into facilities and systems containing cardholder data.\n* 9.3 Physical access for personnel and visitors is authorized and managed.\n* 9.4 Media with cardholder data is securely stored, accessed, distributed, and destroyed.\n* 9.5 Point of interaction (POI) devices are protected from tampering and unauthorized substitution.\n\n### 10. Log and Monitor All Access to System Components and Cardholder Data \n\nNew sections in this requirement regulate the automation of logging, prompt response to failed security controls, as well as how often logs should be reviewed:\n\n* 10.1 Processes and mechanisms for logging and monitoring all access to system components and cardholder data are defined and documented.\n* 10.2 Audit logs are implemented to support the detection of anomalies and suspicious activity, and the forensic analysis of events.\n* 10.3 Audit logs are protected from destruction and unauthorized modifications.\n* 10.4 Audit logs are reviewed to identify anomalies or suspicious activity.\n* 10.5 Audit log history is retained and available for analysis.\n* 10.6 Time-synchronization mechanisms support consistent time settings across all systems.\n* 10.7 Failures of critical security control systems are detected, reported, and responded to promptly.\n\n### 11. Test Security of Systems and Networks Regularly\n\nThis category includes the following:\n\n* a new requirement to manage all applicable vulnerabilities,\n* a new requirement to perform internal vulnerability scans via authenticated scanning,\n* a new requirement to deploy a change-and-tamper-detection mechanism to alert for unauthorized modifications to the HTTP headers and contents of payment pages as received by the consumer browser, [according to PCI SSC](https://listings.pcisecuritystandards.org/documents/PCI-DSS-v3-2-1-to-v4-0-Summary-of-Changes-r1.pdf).\n\nThe full list of the sections included in this requirement is:\n\n* 11.1 Processes and mechanisms for regularly testing security of systems and networks are defined and understood.\n* 11.2 Wireless access points are identified and monitored, and unauthorized wireless access points are addressed.\n* 11.3 External and internal vulnerabilities are regularly identified, prioritized, and addressed.\n* 11.4 External and internal penetration testing is regularly performed, and exploitable vulnerabilities and security weaknesses are corrected.\n* 11.5 Network intrusions and unexpected file changes are detected and responded to.\n* 11.6 Unauthorized changes on payment pages are detected and responded to.\n\n### 12. Support Information Security with Organizational Policies and Programs\n\nThe title modification reflects that the focus is on organizational policies and programs. In PCI DSS version 4, requirements specify that the audited company should, at least every 12 months:\n\n* document and review cryptographic cipher suites and protocols,\n* review hardware and software components,\n* document the PCI DSS scope,\n* review and update the security awareness program, among other requirements, which are presented below.\n\n \n\n* 12.1 A comprehensive information security policy that governs and provides direction for protection of the entity’s information assets is known and current.\n* 12.2 Acceptable use policies for end-user technologies are defined and implemented.\n* 12.3 Risks to the cardholder data environment are formally identified, evaluated, and managed.\n* 12.4 PCI DSS compliance is managed.\n* 12.5 PCI DSS scope is documented and validated.\n* 12.6 Security awareness education is an ongoing activity.\n* 12.7 Personnel are screened to reduce risks from insider threats.\n* 12.8 Risk to information assets associated with third-party service provider (TPSP) relationships is managed.\n* 12.9 Third-party service providers (TPSPs) support their customers’ PCI DSS compliance.\n* 12.10 Suspected and confirmed security incidents that could impact the CDE are responded to immediately.\n\n## What do you have to do now? \n\nYou have to become PCI-DSS version 4 certified until the 31st of March 2024. There are two possible paths for your company:\n\n1. Your company is already PCI-DSS certified, so until the first deadline you must cover some of the new requirements and until the last deadline all of them, or\n2. Your company is not PCI-DSS certified, so you have to start from the beginning with the new PCI-DSS version and fulfill all requirements according to the specified deadlines.\n\nThis new version of PCI DSS brings considerable changes, and the process of achieving compliance with PCI DSS can be long and cumbersome.\n\nCyscale helps companies accelerate their cloud compliance process and ace audits by providing:\n\n* security controls that check if you’re implementing the requirements correctly, and\n* a page for each standard where you can track your progress and history.\n"},"suggestions":[{"node":{"frontmatter":{"authors":"Sabrina Lupșan","categories":["Cloud Security"],"title":"Cloud Security Assessment as a Vital Part of the M&A Process","seoTitle":"Cloud Security Assessment as a Vital Part of the M&A Process","description":"Mergers and acquisitions (M&A) that involve the consolidation of cloud assets open all parties up to financial, regulatory, and reputational risk.  ","seoDescription":"Mergers and acquisitions (M&A) that involve the consolidation of cloud assets open all parties up to financial, regulatory, and reputational risk.","date":"2023-11-30T14:31:42.286Z","featuredpost":true,"permalink":"cloud-security-assessment-vital-part-mergers-acquisitions","featuredimage":{"publicURL":"/static/fd5379064b5e61af846f660f020c5edc/mandaprocess.png","childImageSharp":{"gatsbyImageData":{"layout":"constrained","images":{"fallback":{"src":"/static/fd5379064b5e61af846f660f020c5edc/2c0f5/mandaprocess.jpg","srcSet":"/static/fd5379064b5e61af846f660f020c5edc/41be8/mandaprocess.jpg 205w,\n/static/fd5379064b5e61af846f660f020c5edc/c78f7/mandaprocess.jpg 410w,\n/static/fd5379064b5e61af846f660f020c5edc/2c0f5/mandaprocess.jpg 820w","sizes":"(min-width: 820px) 820px, 100vw"},"sources":[{"srcSet":"/static/fd5379064b5e61af846f660f020c5edc/913d0/mandaprocess.webp 205w,\n/static/fd5379064b5e61af846f660f020c5edc/91660/mandaprocess.webp 410w,\n/static/fd5379064b5e61af846f660f020c5edc/888e2/mandaprocess.webp 820w","type":"image/webp","sizes":"(min-width: 820px) 820px, 100vw"}]},"width":820,"height":460}}},"tableOfContents":true},"rawMarkdownBody":"Although 2023 began with a cautious outlook on M&A activity, analysts predict that the ongoing focus on digital transformation, including innovation around generative AI, is creating dynamic market conditions that could create transformation opportunities for more buoyant merger and acquisitions activity in the near term.   \n\nBut mergers and acquisitions (M&A) are complex processes that involve the consolidation of assets, operations, and cultures, and open all parties up to financial, regulatory, and reputational risk.  \n\nFrom a technology perspective, you not only acquire a company, but also its cybersecurity risks. This is why effective cyber due diligence is essential for all involved, including situations where there will not be a merging or integration of systems, such as with the acquisition of a firm by private equity (PE) for example.  \n\n## Technology: Opportunity or risk? \n\nIn recent years mindsets have shifted, and companies no longer see technology as a cost of doing business as part of the M&A process, but instead as a way of accelerating growth or getting ahead in a market.  \n\n[PWC notes that technology is empowering CEOs to digitalise and transform businesses,](https://www.pwc.com/gx/en/services/deals/trends.html) and AI has changed the game, with a disruptive impact that will create M&A opportunities as both corporates and private equity firms move to acquire new businesses or potentially exit them to monetize returns.  \n\n“We’ve been seeing for some time the growing skills challenge faced by organisations driving acquisition strategies. AI has upped the game here too, with AI talent being one of the scarcest resources to find,” PWC said in its 2023 mid-year update on Global M&A Industry Trends. \n\n[According to Accenture,](https://www.accenture.com/us-en/insights/strategy/mapping-dna-mergers-acquisitions-value) 74% of CEOs see tech integration in M&A as a growth enabler or source of competitive advantage, but only 21% say tech due diligence was conducted for most of their deals over the past three years (to 2022).  \n\nThe obvious issue here becomes even more clear when the same report finds that 96% of CIOs have seen tech due diligence uncover issues or opportunities that made a material impact on the deal.  \n\n## Cybersecurity due diligence in M&A \n\nIt seems to be the case that one aspect that's often overlooked, yet fundamentally critical in the M&A process, is comprehensive planning for the integration of IT systems, including cloud environments.  \n\nIt’s possible that lack of education plays a role here, especially in cases where a company is being acquired for its technology by a more established acquirer built on legacy systems looking to keep pace with the market.  \n\nWhile it may not look complicated from the outside, it is not the case that merging two cloud-based organizations, even when both are using the same cloud provider, will ease the integration.  \n\nThe same is true when the merger will create a hybrid environment mixing on-premises infrastructure with the cloud.  \n\nEither way, integrating a new cloud environment can introduce a lot of new misconfigurations and vulnerabilities in both directions. This article intends to explore why the M&A process needs to be addressed as a cloud security challenge just as much, if not more, as an operational challenge. \n\n## Approaching cybersecurity proactively during M&A  \n\nIdeally, a full cybersecurity audit, including cloud security, will be carried out as part of the due diligence phase of a merger or acquisition. And in cases where there is to be a merging of infrastructure, an audit should again be carried out on day zero of the integration. \n\nThe negative consequences of failing to comprehensively assess your cloud security include: \n\n* Reputational risks and damage to brand \n* Increased costs down the road, either from the integration itself, lost revenue, or fines for compliance failings or security breaches \n* Failure to comply with existing or newly acquired compliance overheads, regional laws and regulations, resulting in fines or increased spend  \n* Overlooked or lost intellectual property, especially relevant due to the boom in AI and associated tech IP \n* Customer impact including increased friction and churn due to broken or problematic systems \n\n## Common misconceptions when integrating a new cloud environment  \n\n* **We both use the same cloud platform so the integration will be easy:** Unique configurations, architectures, and approaches mean that no two cloud environments are alike, even on the same platform. \n* **We’re mostly on-prem so it’s just adding a new technology:** The cloud is a wildly different environment to anything that has come before and even now there is a significant shortage of experience and expertise in the market. Many security problems arise from a lack of education about how to operate safely in the cloud.  \n* **We will audit the cloud environment at a later date:** Due to its public nature, security incidents in the cloud happen in real time. A new zero-day vulnerability in a common software package can be exploited on a mass scale hours after being revealed. A DevOps team spinning up a new instance with a misconfiguration can open your entire infrastructure up to risk. Cloud security posture has to be monitored constantly.  \n* **We can rely on our existing toolset or expertise:** The difference from one cloud platform to another is considerable. They even use different terminology. An expert in AWS is not necessarily an expert in Azure, and security tools may not comprehensively cover multiple environments. The introduction of another cloud is the introduction of another risk, and another visibility challenge.  \n\n## Cloud security risks that come with an M&A event: Real examples \n\n### Data leakage \n\nWhen moving data from one cloud service provider to another, mistakes are bound to happen. In one case a company was moving some sensitive files from AWS S3 buckets to an Azure Storage Account.  \n\nDuring the migration, the team utilized Shared Access Signatures (SAS) in Azure to control access to the stored data. However, due to time constraints and the complexity of the migration process, the team hastily generated SAS tokens with overly broad permissions. \n\nInstead of applying fine-grained access controls to specific resources or limiting access to authorized personnel, the tokens granted broader read and write permissions across a wider range of containers or blobs within the Azure Storage Account. \n\nThis misconfiguration inadvertently exposed sensitive data, including financial records and proprietary algorithms. \n\n**Solution:** Even though a migration can be daunting, take it step by step and understand the best practices of cloud resources you’re using so you can make informed decisions. For example, in the case of storage accounts in Azure, a recommendation is to use RBAC (Role-based access control), rather than SAS signatures.  \n\nIt’s not enough to understand the technology you’ve been using to date, you also need to have a good grasp of the new cloud provider you’re integrating with as every cloud’s approach is different.  \n\n<br class=\"\" />\n\n<a href=\"#data\">\n  <img src=\"/img/data-dash.png\" alt=\"\" title=\"\" class=\"link\" style=\"width:38.0625rem;height:auto;\"/>\n</a>\n\n### Attempting integration of incompatible systems \n\nHarmonizing disparate IT systems, particularly cloud environments, is a formidable challenge in M&A events. Incompatible technologies can create vulnerabilities that malicious actors may exploit.  \n\nIn one case two companies performed encryption for PII differently, and when they ultimately had to merge the sensitive data from two different repositories, the data had to be decrypted and then re-encrypted in a unified manner. This opened up the possibility of several risks – what happens to the data while it’s unencrypted? And where is it stored?  \n\n**Solution:** When it comes to sensitive data, such as PII, time is of the essence but it’s more important to do the work properly, so don’t rush to process it and skip steps. \n\nMinimize the time data is unencrypted. Decrypt the data only when necessary for the merge and re-encrypt it promptly after the process is complete. Moreover, perform the decryption and re-encryption within a secure, controlled environment with restricted access. Use encrypted communication channels and isolated systems to prevent unauthorized access to the unencrypted data. \n\n### Lack of visibility into the extended cloud estate \n\nOne of the big problems that often appears during or after a merger or an acquisition is the lack of visibility into the new or extended cloud estate. When you have to deal with complex infrastructures and unknown environments, security issues can creep in without you noticing. Roles or permissions that should be onboarded or offboarded, redundant VMs that have to be deprovisioned, storage resources that have to be deleted, these are just some examples of cloud resources that may change during an M&A.  \n\nWithout great visibility into a changing environment, mistakes are bound to happen. \n\n**Solution:** To gain much-needed visibility in the cloud, have an active inventory of all your cloud resources and assets, and ideally, the context of how they all interconnect. There are tools which will automatically scan your estate and build an inventory, and some use dashboards to help visualize your cloud infrastructure even across multiple clouds. \n\nFurthermore, perform access reviews to thoroughly understand the permissions users have and whether they are overprovisioned or even redundant.  \n\n### [](#inventory)Expanding compliance and regulatory challenges \n\nM&A events can often cross multiple geographic or legal jurisdictions, each with its own set of compliance requirements. Ensuring alignment with these diverse regulatory landscapes can be a daunting task, and failure to meet compliance standards can lead to severe penalties and legal consequences. \n\nFor example, for some companies, data sovereignty is an important aspect. This means that some companies, due to compliance requirements, have to store all data related to a specific geography on that country’s territory. If one organization acquired another to get access to a new market, they must ensure continued compliance with data sovereignty requirements in the event of merging of infrastructure or face possible legal battles and substantial fines.  \n\nCommon examples include the applicability of US State Privacy Laws, such as the California Consumer Privacy Act, or other state-specific laws. Or the applicability of EU/UK GDPR and other international laws, all of which highlight the necessity for a comprehensive understanding of regulatory environments and meticulous due diligence in M&A scenarios. \n\n**Solution:** When an M&A event happens, ensure that you understand the compliance requirements related to all parties and create a detailed plan to minimize any disruption from the merger or integration process.  \n\nIn terms of data sovereignty, this means understanding how to store data in the country where it’s supposed to be stored.  \n\n## Approaching cloud security in M&A holistically  \n\nAlthough the considerations set out in this article make an M&A event seem daunting, going into the process with an understanding of what to look out for is half the battle. Proper planning and a comprehensive assessment of your cloud infrastructure(s) during both the due diligence process and day zero of the merge will ensure the process runs smoothly and safely. \n\nThe Cyscale platform is designed to help close these security gaps by providing continuous visibility, security controls, and a single dashboard through which to assess your security posture across multiple clouds, including policies and compliance.  \n\n### Preparing for M&A with a cloud security assessment \n\nThe Cyscale cloud security platform is an agentless cloud-native application protection platform (CNAPP) that can help you assess risk during both the due diligence process and day zero of the merger and integration.  \n\n**I﻿nventory**\n\nTo start with, a fully automated security assessment across AWS, Azure, Google Cloud, Alibaba, and Kubernetes assets will provide you with comprehensive insights into your partner’s cloud estate, including all vulnerabilities and misconfigurations, in a matter of minutes.  \n\nThis will give you a full asset inventory and flag any urgent issues that need to be addressed. The Cyscale Graph View means that misconfigurations are analyzed in context, automatically correlating issues that affect compute, data storage and identity assets, determining their real impact on data security and helping prioritize remediation efforts. \n\n<strong id=\"data\">Data</strong>\n\nThe assessment will also identify all data stores and give you valuable information about encryption status for data at rest, encryption keys, as well as identifying any object containers and databases with issues, such as public access, permissive access policies, or lack of deletion protection.  \n\nThis can help ensure your crown jewels are protected, prevent data leakage, and ensure solid foundations for tasks like merging encrypted databases.  \n\n**C﻿ompliance**\n\nIn terms of both planning the integration work and having ongoing proof of compliance, Cyscale has in-app support for dozens of compliance standards, including CIS Cloud Benchmarks, ISO 27001, SOC 2, GDPR (General Data Protection Regulation), HIPAA, PCI DSS (Payment Card Industry Data Security Standard), NIST and more. \n\nAll these compliance standards and controls are analyzed within the Cyscale app, taking into account the unique cloud infrastructures and their related vulnerabilities and misconfigurations.  \n\nThis consolidates your reputation as a transparent, data-focused organization that proves to customers, auditors, and regulatory compliance bodies how robustly it protects Personally Identifiable Information (PII). \n\nAn easy-to-understand report can then be used to make the case for additional funding or remediation requirements, for both technical stakeholders such as CTOs, CISOs, and security engineers, as well as more business-minded executives such as CFOs, CEOs or the Board.  \n\n### See how we build an asset inventory in this video\n\n<p><iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/OQj8Yo71DvQ?si=C4n-svhf206ClYSQ\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen></iframe></p>\n\n<br class=\"\" />\n\n<br class=\"\" />\n\n<a href=\"https://cyscale.com/request-demo/\">\n  <img src=\"/img/assessment-cta.png\" alt=\"\" title=\"\" class=\"link\" style=\"width:25rem;height:12.5rem;\"/>\n</a>\n"}},{"node":{"frontmatter":{"authors":"Sabrina Lupșan","categories":["Cloud Security"],"title":"The Next Era of Security Scoring: CVSS 4.0 vs CVSS 3.1 and What You Need to Know","seoTitle":"CVSS 4.0 vs CVSS 3.1 and What You Need to Know","description":"The Forum of Incident Response and Security Teams (FIRST) officially launched CVSS 4 in early November. Let’s see how CVSS 4 revolutionizes how we look at vulnerabilities and how it compares to CVSS 3.1. ","seoDescription":"The Forum of Incident Response and Security Teams (FIRST) has launched CVSS 4. See how it compares to CVSS 3.1. ","date":"2023-11-20T10:55:36.862Z","featuredpost":true,"permalink":"security-scoring-cvss4-vs-cvss3-need-to-know","featuredimage":{"publicURL":"/static/b0fe4ca7a8cc5a3e4f868c24a738e6fb/cvss4-vs-cvss3.png","childImageSharp":{"gatsbyImageData":{"layout":"constrained","images":{"fallback":{"src":"/static/b0fe4ca7a8cc5a3e4f868c24a738e6fb/2c0f5/cvss4-vs-cvss3.jpg","srcSet":"/static/b0fe4ca7a8cc5a3e4f868c24a738e6fb/41be8/cvss4-vs-cvss3.jpg 205w,\n/static/b0fe4ca7a8cc5a3e4f868c24a738e6fb/c78f7/cvss4-vs-cvss3.jpg 410w,\n/static/b0fe4ca7a8cc5a3e4f868c24a738e6fb/2c0f5/cvss4-vs-cvss3.jpg 820w","sizes":"(min-width: 820px) 820px, 100vw"},"sources":[{"srcSet":"/static/b0fe4ca7a8cc5a3e4f868c24a738e6fb/913d0/cvss4-vs-cvss3.webp 205w,\n/static/b0fe4ca7a8cc5a3e4f868c24a738e6fb/91660/cvss4-vs-cvss3.webp 410w,\n/static/b0fe4ca7a8cc5a3e4f868c24a738e6fb/888e2/cvss4-vs-cvss3.webp 820w","type":"image/webp","sizes":"(min-width: 820px) 820px, 100vw"}]},"width":820,"height":460}}},"tableOfContents":false},"rawMarkdownBody":"For years, security professionals have used the CVSS (Common Vulnerability Scoring System) to rank software vulnerabilities and assess their severity. It is probably the world’s most used scoring system for [vulnerabilities](https://cyscale.com/blog/critical-vulnerabilities-kubernetes-secrets-risk/), and, despite its updates over the years, it has had its shortcomings.  \n\nCVSS 3.0, updated to CVSS 3.1 in 2019, was the most recent and popular version in use for the last eight years. But in early November the Forum of Incident Response and Security Teams (FIRST) [officially launched CVSS 4](https://www.first.org/cvss/v4-0/index.html) This version promises to address many of the shortcomings of its predecessors.  \n\nLet’s see how CVSS 4 revolutionizes how we look at vulnerabilities and why it’s been a long time coming. \n\n## Timeline of CVSS updates \n\n* February 2005: CVSS Version 1 appears. This version received much criticism due to its ambiguous metric system \n* June 2007: CVSS version 2 is launched, this time with less inconsistencies. \n* June 2015: CVSS version 3.0 is published, introducing the concept of “Scope” to mark a difference between separate components of a system \n* June 2019: CVSS version 3.1 clarified concepts and introduced new metrics, making the new CVSS score easier to use \n* 2023: CVSS version 4.0 is released.  \n\nNotice how it's been over eight years since CVSS version 3.0 appeared. And while it had a patch four years in with 3.1, it still had problems.   \n\n## The shortcomings of CVSS 3.0 and 3.1 \n\nLack of granularity was a big challenge for CVSS 3.0 and 3.1. The flat scoring allowed for two vulnerabilities that impacted an IT system differently to be scored the same. For example, a vulnerability that allowed remote code execution without authentication could have been ranked the same as one that required local access with elevated privileges. While both can lead to disaster in a company, the former poses a higher risk due to its potential exploitation.  \n\n[Temporal metrics](https://www.first.org/cvss/v3-1/cvss-v31-specification_r1.pdf) did not have a significant impact on the CVSS score. The three temporal metrics were: \n\n* Exploit code maturity, which refers to the stage of the exploit (whether it’s “functional” or “proof-of-concept”, for example), \n* Remediation level, which measures the quality of available patches or mitigation (some values included “unavailable”, “workaround”, and “official fix”), \n* Report confidence, or the degree of confidence in the existence of the vulnerability and the supplied technical details, with values like “reasonable” or “unknown”. \n\nThe scoring accuracy was also debatable. Often, vulnerabilities were ranked over 7.0 (High or Critical), which meant people frequently saw high-severity vulnerabilities. And while the impact of such vulnerabilities can’t be denied, desensitization became a problem. An accurate score is more important than marking everything as critical, which meant better metrics were needed.  \n\n## So what’s changed with CVSS 4? \n\n[According to FIRST](https://www.first.org/cvss/v4-0/index.html) (The Forum of Incident Response and Security Teams), the entity that manages CVSS, version 4.0 has a finer granularity, achieved by introducing new base metrics and values, which allow for a better scoring system: \n\n* The new Base metric, Attack Requirements (AT), measures the conditions that have to be met for an attack to be possible (basically, how many things need to go wrong before an attack is possible), \n* New Base metric values: User Interaction (UI), which captures if a human is needed (besides the [attacker](https://cyscale.com/blog/compromising-azure-cloud-as-guest/)) to compromise a vulnerable system. The possible values are None (N), Passive (P), and Active (A). For example, if a user needs to give permissions deliberately for the attack to be possible, the User Interaction is Active. \n\nAs we can see, these important metrics allow for a better scoring system for vulnerabilities.  \n\nExploit Maturity (E) now measures more accurately the likelihood of the vulnerability to be exploited; it is a temporal metric based on the current state of the exploitation techniques. This temporal metric allows the CVSS score to reflect the severity of a vulnerability in real-time. Remediation Level and Report Confidence have been retired, and the new threat metrics are more accessible to follow and understand. \n\nEnvironmental metrics allow for more customized and specific vulnerability assessments because they take into consideration the conditions and setups of companies’ environments. This is a “feature” that the previous versions of CVSS lacked. Environmental metrics include: \n\n* Confidentiality, Integrity, and Availability Requirements (CR, IR, AR), which are metrics that allow companies to assign an adequate importance to each factor. For example, if the integrity of data is of utmost importance, they can rate IR as High and the others as Medium or Low, \n* Safety is a metric that shows the impact on human lives. For example, if a vulnerability were to cause a fintech organization business disruption and financial losses, the Safety of that vulnerability could be assessed as Negligible, in comparison to the same one affecting a healthcare organization where life is put in danger, in which case Catastrophic would be a much more appropriate score. \n\n## The relation between CVSS and EPSS: Brothers in arms \n\nAnother scoring system - EPSS (Exploit Prediction Scoring System) - is designed to predict the likelihood of a vulnerability being exploited in the future. It aims to forecast the probability that a vulnerability will be used in a successful attack, helping security teams anticipate and prepare for potential threats.  \n\nIn conjunction with CVSS, EPSS can help prioritize the vulnerability remediation efforts. \n\nEPSS typically considers various factors, including: \n\n* the vulnerability's exploitability,  \n* presence of known exploits,  \n* trends in threat actor behavior, and  \n* historical data to predict the likelihood of exploitation.  \n\n## Benefits of CVSS 4.0\n\nIt is important to understand that CVSS version 4.0 does not apply retroactively. Only the vulnerabilities discovered after the release of CVSS 4.0 will receive the new scoring. \n\nThe new CVSS version 4.0 will bring much sought after value to companies trying to assess their security posture. At Cyscale, we display the CVSS and the EPSS scores to help users better understand their cloud environment. We show the CVSS score of vulnerabilities to highlight which ones should be prioritized. By factoring in the CVSS value, which is now much more relevant, we can help organizations understand risks more accurately and highlight [cloud security infrastructure](https://cyscale.com/blog/cloud-infrastructure-security/) improvements with the highest impact and the lowest level of effort required. \n\n<img src=\"/img/cve-screen.png\" alt=\"CVE vulnerability info card in Cyscale\" title=\"\" class=\" blog-image-shadow \" style=\"width:auto;height:auto;\"/>"}},{"node":{"frontmatter":{"authors":"Sabrina Lupșan","categories":["Cloud Security"],"title":"Critical Confluence Authorization Vulnerability Actively Exploited","seoTitle":"Critical Confluence Authorization Vulnerability Actively Exploited","description":"Atlassian has warned that as of November 6th it has observed 'several active exploits' and reports of threat actors using ransomware in association with a critical Improper Authorization Vulnerability in Confluence Data Center and Server.","seoDescription":"Atlassian warns of  active exploits and threat actors using ransomware in critical Improper Authorization Vulnerability in Confluence Data Center and Server.","date":"2023-11-08T11:55:39.247Z","featuredpost":true,"permalink":"critical-authorization-vulnerability-confluence-exploited","featuredimage":{"publicURL":"/static/d806873ee5f93307e67b021fd9e0128e/confluence-criticalvuln-exploit.jpg","childImageSharp":{"gatsbyImageData":{"layout":"constrained","images":{"fallback":{"src":"/static/d806873ee5f93307e67b021fd9e0128e/2c0f5/confluence-criticalvuln-exploit.jpg","srcSet":"/static/d806873ee5f93307e67b021fd9e0128e/41be8/confluence-criticalvuln-exploit.jpg 205w,\n/static/d806873ee5f93307e67b021fd9e0128e/c78f7/confluence-criticalvuln-exploit.jpg 410w,\n/static/d806873ee5f93307e67b021fd9e0128e/2c0f5/confluence-criticalvuln-exploit.jpg 820w","sizes":"(min-width: 820px) 820px, 100vw"},"sources":[{"srcSet":"/static/d806873ee5f93307e67b021fd9e0128e/913d0/confluence-criticalvuln-exploit.webp 205w,\n/static/d806873ee5f93307e67b021fd9e0128e/91660/confluence-criticalvuln-exploit.webp 410w,\n/static/d806873ee5f93307e67b021fd9e0128e/888e2/confluence-criticalvuln-exploit.webp 820w","type":"image/webp","sizes":"(min-width: 820px) 820px, 100vw"}]},"width":820,"height":460}}},"tableOfContents":false},"rawMarkdownBody":"Atlassian has warned that as of November 6th it has observed 'several active exploits' and reports of threat actors using ransomware in association with a critical Improper Authorization Vulnerability in Confluence Data Center and Server. \n\nCVE-2023-22518, first published on October 31st, 2023, is a Confluence vulnerability that affects all pre-existing versions of Data Center and Server. It was initially assigned a CVSS score of 9.1, which was increased to 10, the maximum, on November 6th.  \n\nThis is the [second critical vulnerability](https://confluence.atlassian.com/security/cve-2023-22515-privilege-escalation-vulnerability-in-confluence-data-center-and-server-1295682276.html) in Atlassian Data Center and Server discovered in the same month, alongside CVE-2023-22515.   \n\n## Confluence Improper Authorization vulnerability in detail \n\nThis vulnerability occurs due to Improper Authorization. The bug enables the attacker to: \n\n* reset a Confluence instance, and to \n* create a Confluence instance administrator account. \n\nThis means the attacker can either reset the entire instance, causing the company to lose data unless it is backed up, or they can steal the data by creating an administrator account.   \n\n### What you need to do: mitigation of the Confluence Improper Authorization vulnerability \n\nSince all versions prior to the attack are affected, [Atlassian urges users](https://confluence.atlassian.com/security/cve-2023-22518-improper-authorization-vulnerability-in-confluence-data-center-and-server-1311473907.html) to immediately patch to the new versions released:  \n\n* 7.19.16, \n* 8.3.4, \n* 8.4.4, \n* 8.5.3, \n* 8.6.1. \n\nIf patching is not possible straight away, remove the instance from being publicly accessible. This is a temporary measure that allows you to gain time by limiting the attack surface – if your instance is not Internet-facing, attackers cannot reach it as easily. \n\nBesides this, it is recommended to back-up your instance. \n\nIf you cannot patch the instance and remove it from the internet, you can apply the following temporary solutions, which you can also find on [Atlassian’s page](https://confluence.atlassian.com/security/cve-2023-22518-improper-authorization-vulnerability-in-confluence-data-center-and-server-1311473907.html): \n\nBlock access to the following endpoints: \n\n* /json/setup-restore.action \n* /json/setup-restore-local.action \n* /json/setup-restore-progress.action \n\nTo do that, on each node, modify /<confluence-install-dir>/confluence/WEB-INF/web.xml and add the following block of code (just before the </web-app> tag at the end of the file): \n\n```\n<security-constraint>\n\t\t<web-resource-collection>\n\t\t\t<url-pattern>/json/setup-restore.action</url-pattern>\n\t\t\t<url-pattern>/json/setup-restore-local.action</url-pattern>\n\t\t\t<url-pattern>/json/setup-restore-progress.action</url-pattern>\n\t\t\t<http-method-omission>*</http-method-omission>\n\t\t</web-resource-collection>\n\t<auth-constraint />\n</security-constraint>\n```\n\nThen, restart your instance.   \n\n### How do you know if you were affected? \n\nIf you cannot login anymore, it could be a sign that your Confluence instance has been compromised. Besides this, look out for: \n\n* requests to /json/setup-restore* in your logs, \n* installed unknown plugins (the malicious plugin web.shell.Plugin was reported, according to Atlassian), \n* corrupted data or encrypted files that were not encrypted before, \n* new and unexpected members of the confluence-administrators group, \n* new and unexpected user accounts. \n\nCyscale customers are already protected, as the [Cyscale cloud security platform](https://cyscale.com/) surfaces assets affected by the Improper Authorization Vulnerability in Confluence Data Center and Server as long as their vulnerability scanner of choice has been updated."}},{"node":{"frontmatter":{"authors":"Sabrina Lupșan","categories":["Compliance"],"title":"Everything You Need to Know about HITRUST Compliance in the Cloud","seoTitle":"Everything to Know about HITRUST Compliance in the Cloud","description":"HITRUST (Health Information Trust) Alliance is the organization that established the HITRUST CSF (Common Security Framework), a framework for managing and securing information in the healthcare industry. This comprehensive framework regulates how healthcare providers and other health businesses handle sensitive data, store it and protect it. \n\nThe HITRUST framework goes above and beyond what HIPAA requires and while its stringency can make it daunting to implement, the benefits of this certification in terms of credibility and assurance are not to be underestimated. \n\nHITRUST CSF is based on ISO/IEC 27001 and 27002 and incorporates more than 50 regulations, standards, and frameworks, thus providing a complete set of requirements and best practices to ensure security. ","seoDescription":"While it can be daunting to implement, HITRUST is considered best-in-class for data security and privacy healthcare certification.","date":"2023-11-07T08:34:54.146Z","featuredpost":true,"permalink":"hitrust-compliance-in-the-cloud","featuredimage":{"publicURL":"/static/5c8091475f4aa54a455a6cb0cdf46dfc/58_blog-cover-image.png","childImageSharp":{"gatsbyImageData":{"layout":"constrained","images":{"fallback":{"src":"/static/5c8091475f4aa54a455a6cb0cdf46dfc/2c0f5/58_blog-cover-image.jpg","srcSet":"/static/5c8091475f4aa54a455a6cb0cdf46dfc/41be8/58_blog-cover-image.jpg 205w,\n/static/5c8091475f4aa54a455a6cb0cdf46dfc/c78f7/58_blog-cover-image.jpg 410w,\n/static/5c8091475f4aa54a455a6cb0cdf46dfc/2c0f5/58_blog-cover-image.jpg 820w","sizes":"(min-width: 820px) 820px, 100vw"},"sources":[{"srcSet":"/static/5c8091475f4aa54a455a6cb0cdf46dfc/913d0/58_blog-cover-image.webp 205w,\n/static/5c8091475f4aa54a455a6cb0cdf46dfc/91660/58_blog-cover-image.webp 410w,\n/static/5c8091475f4aa54a455a6cb0cdf46dfc/888e2/58_blog-cover-image.webp 820w","type":"image/webp","sizes":"(min-width: 820px) 820px, 100vw"}]},"width":820,"height":460}}},"tableOfContents":true},"rawMarkdownBody":"As well as delivering innovation and improving care for customers, organizations operating in the healthcare industry are also required to demonstrate robust processes in security, data protection and privacy, and HITRUST is considered by many the best-in-class for data security and privacy healthcare certification.   \n\n[HITRUST (Health Information Trust) Alliance](https://hitrustalliance.net/) is the organization that established the HITRUST CSF (Common Security Framework), a framework for managing and securing information in the healthcare industry. This comprehensive framework regulates how healthcare providers and other health businesses handle sensitive data, store it and protect it. \n\nThe HITRUST framework goes above and beyond [what HIPAA requires](https://cyscale.com/blog/hipaa-compliance-in-cloud/) and while its stringency can make it daunting to implement, the benefits of this certification in terms of credibility and assurance are not to be underestimated. \n\nHITRUST CSF is based on ISO/IEC 27001 and 27002 and incorporates more than 50 regulations, standards, and frameworks, thus providing a complete set of requirements and best practices to ensure security. \n\nIn this article, we will look at the requirements that apply specifically to healthtech companies with apps and data in the cloud and how to ensure your security and compliance processes are robust enough to pass HITRUST certification. \n\n## HITRUST control categories and how they relate to cloud security \n\nThe standard contains 14 control categories, numbered from 0 to 13: \n\n1. Information Security Management Program \n2. Access control \n3. Human Resources Security \n4. Risk Management \n5. Security Policy \n6. Organization of Information Security \n7. Compliance \n8. Asset Management \n9. Physical and Environmental Security \n10. Communications and Operations Management \n11. Information Systems, Acquisition, Development, and Maintenance \n12. Information Security Incident Management \n13. Business Continuity Management \n14. Privacy Practices \n\nEach control category has one or more objectives, which define the purpose or scope of its requirements, and each Objective contains control references, which encompass a best practice or requirement. There are 49 objectives and 156 control references.  \n\nThis standard can become overwhelming, especially when looking at the 516 pages of version 11.2.0 (the latest one as of November 2023), making implementation a daunting task for small or inexperienced teams. But as we said, the benefits in terms of credibility and increased confidence from your customers cannot be underplayed. Furthermore, there are solutions out there that can do the heavy lifting, such as [ensuring your cloud security controls are in compliance with the framework](https://cyscale.com/products/cloud-security-posture-management/) and alerting you if drifts are detected.  \n\n## Access control for the cloud under HITRUST CSF \n\nThe \"Access control\" category includes recommendations on user management, password policies, [the Least Privilege principle](https://cyscale.com/blog/check-for-least-privilege/), network segregation, session timeout, teleworking, and many others—everything related to [authentication and authorization](https://cyscale.com/blog/iam-best-practices-from-aws-azure-gcp/). \n\n### Effective cloud user password management \n\nImplementing a robust password policy is very important. Imposing a minimum number of characters, as well as the usage of special characters, lowercase and uppercase letters, numbers, and other conditions, is imperative because users tend to want to choose passwords that are easy to remember and type, which increases the risk of having credentials stolen through brute force. Moreover, users should be prohibited from using the same password twice and having old passwords in use. \n\nHow do you implement an effective password policy in the cloud? \n\nTo ensure users implement secure passwords, use the following checklist: \n\n* a minimum length of 14 characters, \n* no password reuse for 24 months, \n* at least a lowercase character, \n* at least an uppercase character, \n* at least a digit, \n* at least one symbol. \n\nAlthough this seems like a lot to consider, it only takes a few minutes to make a great password policy, and for AWS, for example, you can do this in one quick command:  \n\n```\n\naws iam update-account-password-policy --minimum-password-length 14 --password-reuse-prevention 24 --require-lowercase-characters --require-uppercase-characters --require-numbers --require-symbols\n```\n\n### Network Routing Control for cloud infrastructure \n\nRouting controls should be in place to prevent unnecessary connections and information flows across your cloud infrastucture, according to [HITRUST CSF](https://hitrustalliance.net/product-tool/hitrust-csf/). \n\nFirewalls can block unwanted traffic to your cloud, and we recommend filtering traffic by allowlisting IP addresses and ports to reduce the attack surface and further control the access. [CSPM tools such as Cyscale](https://cyscale.com/products/cloud-security-posture-management//) provide a wide range of security controls that granularly verify that firewall rules are written for the most important ports, such as databases. \n\n<img src=\"/img/58_blog-network-controls.png\" alt=\"Network Routing Controls in Cyscale\" title=\"Network Routing Controls in Cyscale\" class=\" blog-image-shadow \" style=\"width:auto;height:auto;\"/>\n\n## Securing Human Resources in the cloud  \n\nThe next category is “Human resources” and focuses on security and compliance processes that apply during the entire lifecycle of an employee in a company, starting from “Prior to Employment”, through “During On-Boarding”, “During Employment”, all the way up to “Termination or Change of Employment”. Appropriate application of policies and procedures should ensure that every employee is set up with relevant levels of entitlement and privilege to cloud systems and infrastructure, and that those entitlements and access are revoked when the employee leaves.  \n\nPoor access management processes can result in overprivileged users that unnecessarily expand your attack surface.  \n\n### Cloud Roles and Responsibilities \n\nUnder “Prior to Employment”, the first requirement is “Roles and Responsibilities”. \n\nThere should be a clear definition of roles according to the existing information security policy. This umbrella covers the protection of data by allowing only authorized access. We have a complete guide on [best practices for IAM in the cloud](https://cyscale.com/blog/iam-best-practices-from-aws-azure-gcp/). \n\nWe’ve put our recommendations into a checklist: \n\n* enable MFA for all users, \n* grant users access to resources, services, and data at group level. You can either directly assign the user permissions or add the user to a group and assign permissions at group level. The second one is recommended because you simplify access management. \n* comply with the Least Privilege principle to only assign the necessary permissions for the required amount of time, and others. \n\n## Compliance controls for cloud assets \n\nJumping to the sixth control category, “Compliance”, there are several technical requirements relevant to cloud infrastructure in the healthcare sector. \n\n### Data Protection in the cloud and Privacy of Covered Information \n\nTo ensure the confidentiality of data, encryption should be enabled for cloud assets that contain sensitive information. This section of the HITRUST standard refers to how encryption should be applied. Here is a [comprehensive guide on data encryption in all its states](https://cyscale.com/blog/types-of-encryption/) - data in use, data in motion, and data at rest.  \n\n### Regulation of Cryptographic Controls \n\nEncrypting data is only effective if the used cryptographic algorithms are industry-recommended. For example, if a vulnerable version of SSL/TLS is deployed, data in transit is unprotected. TLS 1.2 is recommended for securely transporting data. \n\nFor [encryption of data at rest](https://cyscale.com/blog/protecting-data-at-rest/), AES-256 (Advanced Encryption Standard with a key of 256 bits) is the standard. \n\n## Protecting your cloud environment and your code \n\nThe 9th control category, “Communications and Operations Management\" in HITRUST CSF covers segregation of duties, change management, malicious code, back-ups, and many other controls. These are most relevant to cloud security: \n\n### Effective change management in the cloud \n\nTo ensure [comprehensive change management in the cloud](https://cyscale.com/blog/cloud-security-posture-management-cspm-guide/), log and monitor all users' actions. This helps you identify and control changes that occur in the cloud environment. To configure a comprehensive change management process, consider logging changes to: \n\n* firewall rules, \n* IAM policies, \n* routing tables, \n* network gateways, \n* VPCs, \n* SQL instances, and others. \n\n### Controls against malicious code in the cloud \n\nThere are a number of things you can do to protect your cloud apps and data against malware. Scan your VMs for malicious programs and make sure their configurations are secure. In Google Cloud, you can deploy your VMs in a hardened state by enabling the “[Shielded VM](https://cyscale.com/blog/securing-google-cloud-compute-shielded-vm/)\" option to prevent malicious code, such as rootkits and backdoors, from infiltrating your cloud environment. \n\nTurning on Shielded VM in Google Cloud takes just a few clicks. Navigate to the VMs' \"Security\" configurations and tick all boxes under this feature.  \n\n<img src=\"/img/58_blog-shieldedvm.png\" alt=\"Enabling Shielded VM in Google Cloud\" title=\"Enabling Shielded VM in Google Cloud\" class=\" blog-image-shadow \" style=\"width:auto;height:auto;\"/>\n\n## Cloud back-ups best practices \n\nThe “Back-up” category not only recommends performing regular back-ups, but also having recovery procedures in place for highly sensitive cloud resources, such as key vaults.   \n\nMake [cloud data back-ups](https://cyscale.com/blog/cloud-data-security-guide/) and store them separately from the original cloud environment; you can store them in different data centers and regions and use availability zones to ensure that if an unforeseen event occurs at one datacenter, you can still restore a back-up with little to no delay.  \n\nMoreover, enable soft delete for resources that allow this to make sure you’re not deleting a cloud asset that you didn’t mean to delete. For example, Azure Key Vault has soft delete, as well as purge protection. Soft delete allows you to restore the key vault after deletion for a period set by you (between 7 and 90 days), while purge protection will not allow you to purge the key vault (delete forever) manually. You can enable both of these settings quickly with the following command:  \n\n```\n\naz resource update --id <resourceID> --set properties.enablePurgeProtection=true properties.enableSoftDelete=true\n```\n\n### Audit Logging, Monitoring System Use, Protection of Log Information, Administrator and Operator Logs in the cloud \n\nThere are so many control references for logging it can get a bit overwhelming, so let’s break this section apart to make it more manageable. \n\nAudit Logging and Monitoring System Use are quite similar. The first requires you to log user activity and security events, while the latter refers to information processing systems.  \n\nProtection of Log Information recommends that logs are protected from tampering or unauthorized access. Consider securing the bucket or storage account where logs are collected; it is easy for companies to forget to secure logging storage assets because they’re more focused on the sensitive data, especially in regulated sectors like healthcare. \n\nAdministrator and Operator Logs require that high-privilege actions should be logged. For example, the usage of the “root” account in AWS should be logged in order to keep track of who accesses it and why. \n\n## Automating cloud security and HITRUST compliance  \n\nHITRUST CSF is a very complex and detailed compliance standard that can be overwhelming to understand and daunting to implement. What we have covered in this article may only scratch the surface but should serve as a solid foundation on your journey to bringing your cloud compliance in alignment with the framework.  \n\nTo make the overall process more manageable, consider a solution such as the [Cyscale cloud securty platform](https://cyscale.com/) to keep track of your progress, monitor for drifts, and automate security checks that validate your efforts toward achieving HITRUST compliance. Cyscale has over 300 controls that verify your cloud settings against the HITRUST CSF standard and features easily exportable reports to better illustrate your efforts to an auditor, speeding your journey to accreditation. \n\n<img src=\"/img/58_blog-standard-in-cyscale.png\" alt=\"HITRUST in Cyscale\" title=\"HITRUST in Cyscale\" class=\" blog-image-shadow \" style=\"width:auto;height:auto;\"/>"}}]}},"staticQueryHashes":["21106388","2199015418","220583031","3058837307","3355083976","4109069157","632500807","981947644"],"slicesMap":{}}