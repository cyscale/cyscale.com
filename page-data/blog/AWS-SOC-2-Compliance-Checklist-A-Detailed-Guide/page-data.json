{"componentChunkName":"component---src-template-blog-template-js","path":"/blog/AWS-SOC-2-Compliance-Checklist-A-Detailed-Guide/","result":{"pageContext":{"alldata":{"frontmatter":{"authors":"Sabrina Lupșan","categories":["News","Compliance"],"title":"AWS SOC 2 Compliance Checklist: A Detailed Guide","seoTitle":"AWS SOC 2 Compliance Checklist: A Detailed Guide","description":"SOC 2 is a compliance standard that regulates the way data security is handled within B2B (business-to-business) organizations. SOC (Service and Organization Controls) 2 is an international standard developed by AICPA (The American Institute of Certified Public Accountants). \n\nIn this article, we will understand what requirements your company needs to fulfill to obtain the SOC 2 certifications and how to implement them correctly in your AWS environment. ","seoDescription":"AWS SOC 2 Compliance","date":"2022-10-19T10:11:42.781Z","featuredpost":true,"permalink":"AWS-SOC-2-Compliance-Checklist-A-Detailed-Guide","featuredimage":{"publicURL":"/static/db7c98536037b7995ebf25758763897c/26-cyscale-blog-min.webp","childImageSharp":{"gatsbyImageData":{"layout":"constrained","images":{"fallback":{"src":"/static/db7c98536037b7995ebf25758763897c/888e2/26-cyscale-blog-min.webp","srcSet":"/static/db7c98536037b7995ebf25758763897c/913d0/26-cyscale-blog-min.webp 205w,\n/static/db7c98536037b7995ebf25758763897c/91660/26-cyscale-blog-min.webp 410w,\n/static/db7c98536037b7995ebf25758763897c/888e2/26-cyscale-blog-min.webp 820w","sizes":"(min-width: 820px) 820px, 100vw"},"sources":[]},"width":820,"height":460}}},"tableOfContents":null},"rawMarkdownBody":"<!--StartFragment-->\n\nSOC 2 is a compliance standard that regulates the way data security is handled within B2B (business-to-business) organizations. SOC (Service and Organization Controls) 2 is an international standard developed by AICPA (The American Institute of Certified Public Accountants). In the context of **[cloud compliance](https://cyscale.com/blog/cloud-compliance-101-basics-best-practices/)**, understanding SOC 2 requirements in your AWS environment is crucial for data security.\n\nIn this article, we will understand what requirements your company needs to fulfill to obtain the [SOC 2 certifications](https://cyscale.com/blog/soc-2-compliance-in-cloud/) and how to implement them correctly in your AWS environment. \n\n### The requirements \n\nFirstly, the rules within SOC 2 are grouped by 5 TSC (Trust Service Criteria): \n\n1. Security \n2. Availability \n3. Processing Integrity \n4. Confidentiality \n5. Privacy \n\nSOC 2 has 64 mandatory trust service criteria, based on these which the company creates controls to achieve compliance. In this article, we will look at a few of the criteria required, how to implement them in your AWS environment and how to check if your implementation is complete. \n\n##### A checklist \n\n### Requires Additional Authentication or Credentials \n\nMFA (Multi-Factor Authentication) is a mechanism that adds additional steps to the authentication flow and requests supplementary credentials. These credentials can be: \n\n* What you know (for example: a password) \n* What you have (for example: a smart card) \n* What you are (for example: a fingerprint) \n\nTo fulfill this criteria, introduced under the “Security” section, you need to enable MFA when accessing the AWS Management Console. As a result, users will be prompted to enter their username and password (which is the first step, or the first factor of the authentication), and then an authentication code that is sent to their device. A biometrics-enabled device can be used instead as well. \n\nYou can configure MFA for IAM users or the AWS account root user. \n\nSteps to enable MFA in your AWS environment, according to the [documentation](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_mfa_enable.html). \n\n6. Install an authenticator app on your MFA device, use a FIDO2 device or specialized hardware devices such as TOTP tokens. \n7. Enable the MFA device from the AWS Management Console (if using an authenticator app, you can also use AWS CLI or AWS API). \n\nIf your company is using an external identity provider, you have to ensure that MFA is configured for access to the AWS console. \n\n### Restricts Access \n\nThis criteria, in the “Security” TSC refers to restricting traffic to and from any cloud services and assets hosted in the cloud. This includes: \n\n* Closing unnecessarily open ports, \n* Requiring authenticated access to services such as FTP, SMB, and others. \n\nTo close an open port, follow these steps: \n\n1. Log in to the AWS Management Console. \n2. From the EC2 dashboard, on the left pane, click Security Groups. \n3. For each security group, click the Inbound Rules tab, and remove the rules that allow access to the ports you want to close. Repeat the same for the Outbound Rules. \n4. Remember to click save. \n\n### Implements Boundary Protection Systems \n\nThe third requirement we will analyze under the \"Security\" section of SOC refers to the usage of firewalls, DMZs (Demilitarized Zones), IDS, IPS, and others.  \n\nIt is recommended that you secure your cloud infrastructure by using these utilities in order to limit traffic as much as possible and only allow access to resources when necessary. \n\nTo quickly deploy a firewall: \n\n* Log in to the AWS Management Console, \n* Open the Amazon VPC console, create a firewall subnet, and update your VPC Route Tables. \n* Configure the firewall policy by accessing the Amazon VPC console navigation pane under Network Firewall. Choose Firewall policies and add any desired configurations. \n\n### Creates and Maintains Records of System Storage Activities \n\nThis criteria, located under the “Processing Integrity” criteria, checks if logging is implemented in your cloud environment.  \n\nBy logging all user activities, you can: \n\n* Observe [misconfigurations](https://cyscale.com/blog/common-cloud-misconfigurations-how-to-avoid-them/), \n* Identify any suspicious behavior, \n* Detect malicious attacks. \n\nTo enable logging in your AWS infrastructure, you can use Amazon CloudTrail. This solution tracks all actions performed in your cloud environment.  \n\nAmazon CloudTrail can be used together with Amazon CloudWatch to extend monitoring to applications and cloud assets and to analyze their health. \n\n**Amazon CloudTrail is enabled by default in your AWS account.** \n\n### Protects Encryption Keys \n\n[Encryption](https://cyscale.com/blog/types-of-encryption/) is essential to ensure confidentiality of data. In the “Confidentiality” section of SOC 2, we have criteria for both encryption and key management.  \n\nIf key management is neglected, then encryption becomes useless. Keys must be generated, stored, used and destroyed safely to protect your data. \n\nAmazon offers a complete solution for proper key management. AWS KMS (AWS Key Management System) helps you properly deal with all of the processes in a cryptographic key’s lifecycle. \n\nAccording to AWS, this service can be used:  \n\n* Through the AWS Management Console,  \n* Using the AWS KMS APIs. \n\n### Finally, check your implementations \n\nBesides the criteria presented in this article, there are many more. \n\nAlthough so many requirements can quickly become overwhelming, Cyscale can help you easily keep track of what you’ve correctly implemented and what requires your attention on our [SOC 2](https://cyscale.com/blog/soc-2-vs-ISO-27001-SaaS/) compliance page. \n\nMany technical controls in Cyscale can be mapped to SOC 2 criteria to provide visibility in your cloud environment and prove compliance. \n\nA few examples of controls that apply to the presented requirements (and more) are: \n\n* Ensure all users have MFA configured. \n* Ensure CloudTrail is enabled in all regions. \n* Ensure no security groups allow ingress from 0.0.0.0/0 to port 22 (SSH). \n\n<!--EndFragment-->"},"suggestions":[{"node":{"frontmatter":{"authors":"Sabrina Lupșan","categories":["News"],"title":"Inside the Mind of an Attacker: How Contextual Security Can Save Your Cloud","seoTitle":"Inside the Mind of an Attacker: How Contextual Security Can Save Your Cloud","description":"The future of cloud security is contextual security. To fully understand how secure an asset is, you need to understand what users have permissions to interact with it and what resources it communicates with. It might seem that a VM is completely secure because you put it behind a firewall, but if a compromised user can access it, it’s game over. \n\nThe perfect recipe for contextual security is a Cloud Security Knowledge Graph. Based on it, we can represent in a visual and interactive way how cloud resources interact, what kind of relations they have, what users have permissions to read/write on them, and so on.  \n\nTo illustrate my point, I will show you some scenarios where the difference between a secure cloud and a breach is made by fixing misconfigurations and limiting users' access. These measures are easier to identify using a graph, because it helps you understand the risks your environment is exposed to.  ","seoDescription":"The future of cloud security is contextual security. To fully understand how secure an asset is, you need to understand what users have permissions to interact with it and what resources it communicates with. It might seem that a VM is completely secure because you put it behind a firewall, but if a compromised user can access it, it’s game over.   The perfect recipe for contextual security is a Cloud Security Knowledge Graph. Based on it, we can represent in a visual and interactive way how cloud resources interact, what kind of relations they have, what users have permissions to read/write on them, and so on.    To illustrate my point, I will show you some scenarios where the difference between a secure cloud and a breach is made by fixing misconfigurations and limiting users' access. These measures are easier to identify using a graph, because it helps you understand the risks your environment is exposed to.  ","date":"2023-09-19T09:01:02.199Z","featuredpost":true,"permalink":"contextual-security-google-cloud","featuredimage":{"publicURL":"/static/7fbef8aa4e6e383ec434207181f0d892/blog_54-cover.png","childImageSharp":{"gatsbyImageData":{"layout":"constrained","images":{"fallback":{"src":"/static/7fbef8aa4e6e383ec434207181f0d892/2c0f5/blog_54-cover.jpg","srcSet":"/static/7fbef8aa4e6e383ec434207181f0d892/41be8/blog_54-cover.jpg 205w,\n/static/7fbef8aa4e6e383ec434207181f0d892/c78f7/blog_54-cover.jpg 410w,\n/static/7fbef8aa4e6e383ec434207181f0d892/2c0f5/blog_54-cover.jpg 820w","sizes":"(min-width: 820px) 820px, 100vw"},"sources":[{"srcSet":"/static/7fbef8aa4e6e383ec434207181f0d892/913d0/blog_54-cover.webp 205w,\n/static/7fbef8aa4e6e383ec434207181f0d892/91660/blog_54-cover.webp 410w,\n/static/7fbef8aa4e6e383ec434207181f0d892/888e2/blog_54-cover.webp 820w","type":"image/webp","sizes":"(min-width: 820px) 820px, 100vw"}]},"width":820,"height":460}}},"tableOfContents":false},"rawMarkdownBody":"When public cloud providers such as Google Cloud and AWS first appeared, their benefits were hard to ignore. And although companies were reluctant to put their infrastructure and applications in the hands of a different company, cloud solutions have grown in popularity.  \n\nWith more and more complex setups, it is hard for an organization to understand and keep track of the implication of every resource they have in the cloud, the risk it poses to the entire infrastructure or how many other assets it impacts. \n\nI believe the future of cloud security is contextual security. To fully understand how secure an asset is, you need to understand what users have permissions to interact with it and what resources it communicates with. It might seem that a VM is completely secure because you put it behind a firewall, but if a compromised user can access it, it’s game over. \n\nThe perfect recipe for contextual security is a [Cloud Security Knowledge Graph](https://cyscale.com/blog/security-knowledge-graph-integrations/). Based on it, we can represent in a visual and interactive way how cloud resources interact, what kind of relations they have, what users have permissions to read/write on them, and so on.  \n\nTo illustrate my point, I will show you some scenarios where the difference between a secure cloud and a breach is made by fixing misconfigurations and limiting users' access. These measures are easier to identify using a graph, because it helps you understand the risks your environment is exposed to.  \n\n## Case Study \n\nLet’s assume we use BigQuery in Google Cloud for a health analytics application. We have a table that contains some datasets of patients from a hospital, including PII data. These datasets are very valuable and cannot fall in the hands of outsiders. \n\nNow, we want to make sure that the data is securely stored. We look at the BigQuery table’s graph to see if the table poses any risk and, low and behold, it does! \n\n<img src=\"/img/blog_54-graf-0.png\" alt=\"Contextual security through the graph\" title=\"Contextual security through the graph\" class=\" blog-image-shadow \" style=\"width:auto;height:auto;\"/>\n\nWe can observe, in the image above, that no less than 7 Cloud Functions and 4 VMs can access it across 9 Service Accounts that have permissions on the BigQueryTable, as well as 7 IAM Users. Does this alarm you? It should. \n\nBut just because a VM, or a function, can have access to the table does not mean anything, right? Wrong! I’m going to show you exactly how bad things can get in this situation. I’m going to put my hacker shoes on and show you potential scenarios that might lead to data being stolen. \n\n  \n\n### Scenario 1: compromised VM \n\nOut of the 4 VMs that we see in the image, one is an Internet-facing one. The compute instance “dev-1” hosts an application that has the Log4J vulnerability, a classic. A hacker leverages the vulnerability and gains access to the instance, being able to execute commands on it, and the disaster begins. **Because a Service Account is associated to the VM, the attacker has that account’s permissions.** In our scenario, the application running on the Instance needs to process data in the dataset, so the associated Service Account was given the roles/bigquery.dataEditor permission. \n\nMoreover, in a VM, credentials used to manually authenticate are stored on the Compute Instance after the first time a user authenticates as a Service Account, in */.config/gcloud/credentials.db*, as you can see in the image below. \n\n<img src=\"/img/blog_54-ss1.png\" alt=\"Credentials stored in Google Cloud VM\" title=\"Credentials stored in Google Cloud VM\" class=\" blog-image-shadow \" style=\"width:auto;height:auto;\"/>\n\nThis is a standard location and it stores the last private key that was used to authenticate as a Service Account.  You can also find access tokens in the same folder, in access_tokens.db. Moreover, you can see any other private keys used previously (which may still be valid, if they were not deleted in the Google Cloud Console) in */.config/gcloud/legacy_credentials/<serviceAccountEmail>/adc.json*. \n\nLooking at the file credentials.db, we notice that the information is not stored in the standard format for a private key for a Service Account. With a few adjustments, we obtain a valid private key that we then use to authenticate. To authenticate as a Service Account with a private key, simply use the following command:  \n\n```\ngcloud auth activate-service-account <serviceAccountEmail> --key-file=<keyFile>\n```\n\n<img src=\"/img/blog_54-ss2.png\" alt=\"Authenticating using the secret key\" title=\"Authenticating using the secret key\" class=\" blog-image-shadow \" style=\"width:auto;height:auto;\"/>\n\nHowever, what happens when no one has authenticated as a Service Account on the VM? There is no *credentials.db* file (actually, there is no *.config* folder).  \n\nDevelopers tend to find ways to do things faster, and they get comfortable. It is not uncommon to find secrets in environment variables or plain-text files. If a developer were to store a secret key file on the Compute Instance, we could abuse it. Simply log in using the private RSA key file: \n\n<img src=\"/img/blog_54-ss3.png\" alt=\"Private key on the VM\" title=\"Private key on the VM\" class=\" blog-image-shadow \" style=\"width:auto;height:auto;\"/>\n\n<img src=\"/img/blog_54-ss4.png\" alt=\"Authenticating as Service Account\" title=\"Authenticating as Service Account\" class=\" blog-image-shadow \" style=\"width:auto;height:auto;\"/>\n\nChecking the current permissions, and… bingo! roles/bigquery.dataEditor is one of the available roles. \n\n<img src=\"/img/blog_54-ss5.png\" alt=\"Service Account with roles/bigquery.dataEditor\" title=\"Service Account with roles/bigquery.dataEditor\" class=\" blog-image-shadow \" style=\"width:auto;height:auto;\"/>\n\nIf we expand the graph’s nodes that we used in this chain of attacks, you can clearly see the attack path: \n\n<img src=\"/img/blog_54-primul-graf.png\" alt=\"Attack path through the VM\" title=\"Attack path through the VM\" class=\" blog-image-shadow \" style=\"width:auto;height:auto;\"/>\n\n\n\nWith the bigquery.dataEditor role, the attacker can now: \n\n* view data and metadata, \n* modify data and metadata, \n* delete tables. \n\n This breach could be avoided by: \n\n* ensuring the VM does not have vulnerabilities and patching the Log4J one, \n* isolating the Compute Instance from the Internet by closing the exposed port, if possible, \n* restricting the Service Account’s permissions as much as possible \n* making sure secrets are cleared from the VM files.   \n\n### Scenario 2: compromised user \n\nExpanding the Users node, we see that there are three users that have access to the BigQueryTable.  \n\n<img src=\"/img/blog_54-ultimul-graf.png\" alt=\"Attack path through user compromise\" title=\"Attack path through user compromise\" class=\" blog-image-shadow \" style=\"width:auto;height:auto;\"/>\n\nIf a hacker were to take over any of those accounts, for example, by stealing credentials, our customer’s data would be at risk. If one of the three users that have access to the table does not have MFA enforced, then the attacker can compromise the account.  \n\nEnabling MFA would prevent this. \n\nBesides this, a common mistake is focusing on protecting your environment from the outside, and forgetting about your own users. If one of the employees in the organization has too many permissions, they may produce damage without intention. By exploring the account and looking at resources, a user can accidentally modify or delete an asset. \n\nThis is why the Least Privilege Principle is so important – always limit the users’ access as much as possible and only assign the necessary permissions. \n\n  \n\nWe believe that context is the future of cloud security. We’ve seen how the most simple relations between assets can be leveraged by attackers to take over cloud assets; and if your customers’ data is stored in those assets, the greater the prize is for hackers – and they will do anything to get it."}},{"node":{"frontmatter":{"authors":"Sabrina Lupșan","categories":["News"],"title":"Cyber Safety Review Board Rings Alarm Bells for Shared Responsibility in the Cloud","seoTitle":"Cyber Safety Review Board Rings Alarm Bells for Shared Responsibility in the Cloud","description":"The Cyber Safety Review Board (CSRB) of the Department of Homeland Security is now embarking on its third cyber vulnerability review since the body was created in February 2022. During August, when lots of people were on summer vacation, a critical vulnerability in the authentication process for Azure Active Directory was exposed.\nAs with the two vulnerabilities the CSRB reviewed previously - Log4J in July of 2022 and Lapsus$ also in August of 2023 – the Azure Active Directory vulnerability highlights significant challenges in terms of remediation, due to the sprawling nature of cloud resources and their direct and indirect dependencies on assets that might, or might not, be affected.  ","seoDescription":"The Cyber Safety Review Board (CSRB) of the Department of Homeland Security is now embarking on its third cyber vulnerability review since the body was created in February 2022. During August, when lots of people were on summer vacation, a critical vulnerability in the authentication process for Azure Active Directory was exposed. As with the two vulnerabilities the CSRB reviewed previously - Log4J in July of 2022 and Lapsus$ also in August of 2023 – the Azure Active Directory vulnerability highlights significant challenges in terms of remediation, due to the sprawling nature of cloud resources and their direct and indirect dependencies on assets that might, or might not, be affected.","date":"2023-09-07T09:02:20.333Z","featuredpost":true,"permalink":"cyber-safety-review-board-on-cloud-security","featuredimage":{"publicURL":"/static/87ae3652b6af1c7dbe17d3182650a181/53_blog-csrb.jpeg","childImageSharp":{"gatsbyImageData":{"layout":"constrained","images":{"fallback":{"src":"/static/87ae3652b6af1c7dbe17d3182650a181/2c0f5/53_blog-csrb.jpg","srcSet":"/static/87ae3652b6af1c7dbe17d3182650a181/41be8/53_blog-csrb.jpg 205w,\n/static/87ae3652b6af1c7dbe17d3182650a181/c78f7/53_blog-csrb.jpg 410w,\n/static/87ae3652b6af1c7dbe17d3182650a181/2c0f5/53_blog-csrb.jpg 820w","sizes":"(min-width: 820px) 820px, 100vw"},"sources":[{"srcSet":"/static/87ae3652b6af1c7dbe17d3182650a181/913d0/53_blog-csrb.webp 205w,\n/static/87ae3652b6af1c7dbe17d3182650a181/91660/53_blog-csrb.webp 410w,\n/static/87ae3652b6af1c7dbe17d3182650a181/888e2/53_blog-csrb.webp 820w","type":"image/webp","sizes":"(min-width: 820px) 820px, 100vw"}]},"width":820,"height":460}}},"tableOfContents":false},"rawMarkdownBody":"The Cyber Safety Review Board (CSRB) of the Department of Homeland Security is now embarking on its third cyber vulnerability review since the body was created in February 2022. During August, when lots of people were on summer vacation, a critical vulnerability in the authentication process for Azure Active Directory was exposed.  \n\nAs with the two vulnerabilities the CSRB reviewed previously - Log4J in July of 2022 and Lapsus$ also in August of 2023 – the Azure Active Directory vulnerability highlights significant challenges in terms of remediation, due to the sprawling nature of cloud resources and their direct and indirect dependencies on assets that might, or might not, be affected.  \n\nEven for large organizations with sizeable security teams, navigating a complex dependency graph to understand which resources have dependencies on affected assets is a challenge. For SaaS startups with perhaps a handful of people building out their technology stack, such investigations are impossible.  \n\nAs a result, the findings of the CSRB’s reports and its recommendations for best practice build a strong case for contextually aware cloud security practices.  \n\n## How does this affect you (and everyone else)? \n\nIn its most recent announcement, the CSRB made it clear that identity access management (IAM) and authentication are in focus due to being one of the most vulnerable areas in a cloud environment. Identity management vulnerabilities are a key entry point for hackers, and SaaS companies and large enterprises are falling victim repeatedly. From IAM misconfigurations that CTOs and developers miss when setting up and managing their environment, to cloud service provider bugs, this area needs critical attention to ensure safe and secure cloud environments. \n\n## Did I just say cloud service provider bugs?  \n\nYes, I did. The incident in question that kicked off this latest review less than two months ago, was the discovery of a critical vulnerability in the authentication process for Azure Active Directory (soon to be renamed Microsoft Entra ID) that was allowing hackers to fabricate authentication tokens. Around 25 organizations including US government entities were impacted by this bug before it was discovered, with attackers stealing mailbox data and obtaining access to sensitive emails.  \n\nFor cloud native organizations building their entire business on public cloud platforms, this shared responsibility model is one of the most misunderstood (or ignored) aspects.  \n\nWhen the terms & conditions say that the cloud is a ‘shared responsibility’, it really is. It’s the cloud service provider’s job to secure the platform or infrastructure, and your job to secure the apps and data, and if one party fails (in this case, the cloud service provider – Microsoft Azure), your company, and potentially your customers and partners, are left exposed. \n\nThe CSRB’s previous two reports on [Log4j](https://www.cisa.gov/resources-tools/resources/csrb-log4j-key-findings-and-recommendations-summary) and [Lapsus$](https://www.cisa.gov/resources-tools/resources/review-attacks-associated-lapsus-and-related-threat-groups-executive-summary) have been very thorough and made some great recommendations that we hope will advance cybersecurity practices, which is why we’re eagerly waiting for CSRB’s report on cloud security. It not only promises a thorough set of recommendations for CISOs, CTOs, and developers, but it will also sound the alarm for cloud service providers.  \n\nAlthough the Cyber Safety Review Board has no regulatory power, its findings are transmitted to US President Biden and used to develop Executive Orders, so we can expect to see its influence in regulations such as [HIPAA](https://cyscale.com/blog/hipaa-compliance-in-cloud/), and perhaps best practices adopted into SOC 2, ISO 27001, and others. \n\n## The importance of contextual security \n\nWhat’s become evident is that simply informing businesses and cloud providers of the discovery of far-reaching misconfigurations and vulnerabilities is not enough. In fact, the potential workload created by responding to these challenges can be just as dangerous in that it can consume all an organization’s security resources.  \n\nContext is important and [contextual security](https://cyscale.com/blog/security-knowledge-graph-integrations/) is the most significant development in cloud infrastructure security in recent years. Identifying that a VM has the Log4J vulnerability is not the same as saying that an Internet-facing VM has the same vulnerability. Of course, they are both just as vulnerable, but if the first VM is running in a private network, the severity is not as high as for the one accessible from the Internet. The risk is not the same, but the alert will be, and we all know alert fatigue is a very real problem.  \n\nWrong prioritization, or no prioritization at all, can make all the difference between a secure cloud environment and a breach."}},{"node":{"frontmatter":{"authors":"Andrei Ștefănie","categories":["News"],"title":"IPv4 Billing Changes in AWS: Impact on Cloud Costs & Security","seoTitle":"IPv4 Billing Changes in AWS: Impact on Cloud Costs & Security","description":"Explore how AWS's new IPv4 billing changes, effective from February 2024, will influence both your financial and security strategies. Understand the implications for small startups to large enterprises, from cost optimization to IPv6 transition, NAT64, and security enhancements.","seoDescription":"Explore how AWS's new IPv4 billing changes, effective from February 2024, will influence both your financial and security strategies. Understand the implications for small startups to large enterprises, from cost optimization to IPv6 transition, NAT64, and security enhancements.","date":"2023-08-31T09:03:04.258Z","featuredpost":true,"permalink":"aws-ipv4-impact-on-cloud-costs-and-security","featuredimage":{"publicURL":"/static/e4eb30151713435e89ec3c37c4832ca7/aws-ipv4-article.png","childImageSharp":{"gatsbyImageData":{"layout":"constrained","images":{"fallback":{"src":"/static/e4eb30151713435e89ec3c37c4832ca7/2c0f5/aws-ipv4-article.jpg","srcSet":"/static/e4eb30151713435e89ec3c37c4832ca7/41be8/aws-ipv4-article.jpg 205w,\n/static/e4eb30151713435e89ec3c37c4832ca7/c78f7/aws-ipv4-article.jpg 410w,\n/static/e4eb30151713435e89ec3c37c4832ca7/2c0f5/aws-ipv4-article.jpg 820w","sizes":"(min-width: 820px) 820px, 100vw"},"sources":[{"srcSet":"/static/e4eb30151713435e89ec3c37c4832ca7/913d0/aws-ipv4-article.webp 205w,\n/static/e4eb30151713435e89ec3c37c4832ca7/91660/aws-ipv4-article.webp 410w,\n/static/e4eb30151713435e89ec3c37c4832ca7/888e2/aws-ipv4-article.webp 820w","type":"image/webp","sizes":"(min-width: 820px) 820px, 100vw"}]},"width":820,"height":460}}},"tableOfContents":false},"rawMarkdownBody":"Beginning February 1, 2024, Amazon Web Services (AWS) will usher in a significant change by charging for public IPv4 addresses. Whether you are a seasoned cloud engineer or a business decision-maker this shift by one of the world's leading cloud providers will likely have broad implications.\n\nAWS's decision aligns with other major cloud providers that have adopted similar practices, reflecting an industry-wide trend. AWS's involvement is particularly significant given its substantial market presence, and it will likely influence various organizations across different scales and industries.\n\nIn terms of your business and the tech landscape, this change goes beyond additional costs, with the new model affecting network planning, igniting considerations about transitioning to IPv6, and opening opportunities to reassess cloud security strategies.\n\n## The Financial Implications of AWS's New Charging Model for IPv4 Addresses\n\n### Detailed Explanation of Pricing\n\nStarting February 1, 2024, AWS will charge for all public IPv4 addresses at a rate of 0.5 cents per hour. While this figure may initially seem inconsequential, it is vital to understand how swiftly it can accumulate. \n\nFor small businesses or individual projects, the impact might be marginal. Our example considers a small organization with 10 public IP addresses:\n\n*10 IPs x 0.5 cents per hour x 730 hours in a month = $36.5 per month, or $438 per year.*\n\nContrast this with an enterprise operating with 11,000 public IPs: \n\n*11,000 IPs x 0.5 cents per hour x 730 hours in a month = $40,150 per month, or nearly **$500,000 per year**.*\n\n1﻿1,000 IP addresses might sound like an overestimation, but we are seeing organizations with tens of thousands of EC2 instances. Assuming 20-30% of these need public IPs is not out of reach.\n\nThis striking difference highlights the importance of understanding and planning for the financial consequences, particularly for organizations heavily reliant on public IPv4 addresses.\n\n### Budget Considerations for Organizations\n\nAWS's decision to charge for public IPv4 addresses extends beyond a simple financial concern; it prompts businesses to reevaluate their entire approach to IP address utilization. Key considerations include:\n\n* **Reviewing existing usage:** Analyzing current usage patterns is essential for predicting additional costs and identifying opportunities for optimization.\n* **Considering IPv6 migration:** Some may find transitioning to IPv6 a cost-effective alternative, though it must be balanced against compatibility and technical constraints.\n* **Utilizing tools and insights:** Automated security platforms like Cyscale can offer comprehensive insights into your inventory of public IP addresses, helping you understand the big picture and delivering insight that can aid in cost control.\n\nThis policy change serves as a catalyst for reimagining how organizations approach cloud infrastructure financially. The ripple effects will be felt differently across the spectrum, but the core message is clear: understanding, planning, and adapting will be pivotal in navigating this change without unforeseen financial setbacks.\n\nWhether you are a decision-maker concerned about the bottom line or a cloud professional tasked with optimization, these changes necessitate close scrutiny and proactive planning.\n\n## Technical Aspects and Considerations\n\n### Impact on EC2 Instances, RDS, EKS, etc.\n\nThe new charging model impacts a broad array of resources, including EC2 instances, RDS, EKS, load balancers, and more. Managing complex architectures now entails a new layer of complexity, possibly requiring a reassessment of networking strategies and configurations.\n\n### The Complexity of Managing Multiple IP Addresses per Resource\n\nSome resources may have more than one public IPv4 address. Managing these involves not just technical configuration but also financial planning. Understanding how these multiple IPs interact within your infrastructure and contribute to overall costs is essential. \n\n### Discussion on the Feasibility and Challenges of Switching to IPv6\n\nThe move to IPv6 seems logical, but it's not devoid of challenges. Compatibility with services and APIs managed by others might become a hurdle. AWS has made progress, such as enabling communication from EC2 to Lambda functions over IPv6 (note that you cannot reach an EC2 instance from Lambda over IPv6), yet some cases may still hinder a complete switch.\n\n### IPv4 to IPv6 Transition Mechanisms\n\nDuring the transition, Network Address Translation (NAT), specifically NAT64, becomes vital. This mechanism translates IPv6 to IPv4 addresses, bridging communication between newer IPv6 systems and legacy IPv4 systems. Understanding and utilizing NAT64 is crucial for modernizing without losing functionality. Equally important is DNS64 which performs the translation at the DNS level. More specifically, in the case a domain is mapped only to an IPv4 address (i.e., it only has A records), DNS64 translates an A record (specific to IPv4) to an AAAA record (specific to IPv6), thus allowing an IPv6 client to reach an IPv4 server.\n\n### Proficiency with Security Groups and Other Network Considerations\n\nWith all IPv6 addresses being public, mastering security groups and leveraging egress-only internet gateways become even more critical. Properly configuring security rules and comprehending their interaction with different IP versions plays a significant role in upholding security and functionality within your network.\n\nThe new AWS charging model for public IPv4 addresses is more than a financial consideration. It intertwines with a multifaceted web of technical aspects that must be thoughtfully navigated. Embracing the right strategies and tools can pave the way for a seamless transition, preserving both efficiency and budget.\n\n## Security Implications\n\n### Reevaluation of Network Exposure\n\nThe new charges for public IPv4 addresses might encourage organizations to scrutinize network exposure more closely. Reducing the number of public IPs could be both a cost-saving measure and a strategy to enhance security by limiting the **attack surface.**\n\n### Importance of Secure Configuration in IPv6 Transition\n\nTransitioning to IPv6, while potentially cost-effective, demands careful consideration of security configurations. Understanding IPv6 security nuances is vital in safeguarding systems during and after the shift. While NAT64 facilitates communication between IPv6 and IPv4 systems, it also introduces unique security considerations. Proper implementation and continuous vigilance are required to ward off vulnerabilities that might arise during the translation process. In practice, most organizations will leverage the AWS NAT Gateway which already supports NAT64 and DNS64 (through Route 53) so our responsibility includes proper route configuration and keeping DNS resolvers up to date. However, other organizations might choose to deploy their own NAT instance to optimize cost.\n\n### Tools and Platforms for Security Management\n\nAutomated cloud security platforms like Cyscale can play a pivotal role in this secure transition. By providing insights into all public IP addresses, their attachments, and alternative communication paths, they can facilitate a more secure and cost-effective migration. Utilizing specialized security platforms and tools is fundamental to maintaining control over complex cloud environments.\n\nThis new AWS charging model for public IPv4 addresses is more than a technical and financial hurdle; it's an opportunity to rethink and potentially enhance security strategies. By understanding the interplay between IPv4 and IPv6, the role of the transition mechanisms, and the necessity of proper security configurations, organizations can navigate this transition without compromising security.\n\n## The Role of Cloud Security Platforms\n\nAutomated cloud security platforms like Cyscale are more than just a reactive measure to changes like AWS's new charging model for public IPv4 addresses; they represent a proactive approach to modern cloud infrastructure management. By providing tools that cut across cost optimization, security enhancement, migration planning, and collaboration, these holistic solutions enable organizations to thrive in an evolving cloud landscape."}},{"node":{"frontmatter":{"authors":"Sabrina Lupșan","categories":["CSPM"],"title":"Securing VMs in Google Cloud: Shielded VM and Other Features You Never Knew Existed ","seoTitle":"Securing VMs in Google Cloud: Shielded VM","description":"Best practices for securing VMs in Google Cloud, including the Shielded VM feature description and practical steps to enable it.","seoDescription":"Best practices for securing VMs in Google Cloud, including the Shielded VM feature description and practical steps to enable it.","date":"2023-08-23T15:16:04.584Z","featuredpost":true,"permalink":"securing-google-cloud-compute-shielded-vm","featuredimage":{"publicURL":"/static/f8641a2b38562362996601473551a92f/51-cyscale-blog-min.jpg","childImageSharp":{"gatsbyImageData":{"layout":"constrained","images":{"fallback":{"src":"/static/f8641a2b38562362996601473551a92f/2c0f5/51-cyscale-blog-min.jpg","srcSet":"/static/f8641a2b38562362996601473551a92f/41be8/51-cyscale-blog-min.jpg 205w,\n/static/f8641a2b38562362996601473551a92f/c78f7/51-cyscale-blog-min.jpg 410w,\n/static/f8641a2b38562362996601473551a92f/2c0f5/51-cyscale-blog-min.jpg 820w","sizes":"(min-width: 820px) 820px, 100vw"},"sources":[{"srcSet":"/static/f8641a2b38562362996601473551a92f/913d0/51-cyscale-blog-min.webp 205w,\n/static/f8641a2b38562362996601473551a92f/91660/51-cyscale-blog-min.webp 410w,\n/static/f8641a2b38562362996601473551a92f/888e2/51-cyscale-blog-min.webp 820w","type":"image/webp","sizes":"(min-width: 820px) 820px, 100vw"}]},"width":820,"height":460}}},"tableOfContents":false},"rawMarkdownBody":"While assessing the controls suggested by the CIS Benchmark 2.0.0 for Google Cloud, I stumbled upon an interesting recommendation: to launch compute instances with Shielded VM enabled. When you think of VM security, you think of open management ports, outdated OS versions, unencrypted VM disks and so on. So what is Shielded VM and why do you need to enable it ASAP? \n\n\n\n## Shielded VM \n\nShielded VM, in a cloud computing context, is a security solution that hardens VMs deployed in Google Cloud. This measure protects them against rootkits and bootkits, which are essentially malware that infect and obtain persistence in VMs, allowing attackers to infiltrate in the cloud environment through the Compute Instances. It’s like a magical portal into your entire cloud infrastructure, which, if created, opens on command for the hackers.    \n\nThe advantage of this setting is that, with just a few clicks, you significantly increase the protection of your [Google Cloud](https://cyscale.com/use-cases/gcp-cloud-security/) VM by providing it with: \n\n* Secure Boot, \n* vTPM, \n* Integrity monitoring. \n\n**Secure Boot** is a feature that verifies the digital signature of all boot components of a virtual machine. In this way, if any signature verification fails, that boot process is halted, thus ensuring that only authentic software is run on the system. This means that you cannot use any custom drivers on the machine, because if you do, it will cause the VM to not boot anymore. If a failure occurs, the user receives two errors:\n\n* **UEFI: Failed to load image**, and \n* **Status: Security Violation.** \n\n**vTPM, or Virtual Trusted Platform Module,** is a computer chip specialized in protecting secrets. To understand what vTPM is, let’s break it down. TPM, or Trusted Platform Module, is a hardware component that stores cryptographic keys, passwords, and other sensitive data securely. By virtualizing it, the same result is achieved, but on a software level, meaning that VMs in the cloud can also be protected using a TPM, even though at hardware-level they may share components with other VMs. \n\nvTPM also introduces **Measure Boot**, a mechanism that checks the integrity of the VM’s components. When the VM first boots, Measure Boot establishes an **integrity policy** baseline by calculating hashes of the components, concatenating them and rehashing them into a final hash to guarantee a sound integrity monitoring. Then, every time the VM boots up, the hash is recalculated and checked against the initial one, thus being able to tell if there are any changes in the boot process. \n\nThe integrity policy baseline needs to be updated if changes appear, such as a system update. \n\nThrough the process described, **integrity monitoring** is achieved using Shielded VM and the Measure Boot mechanism. \n\n\n\n### How to enable Shielded VM \n\nTo enable Shielded VM, you can do it through the Google Cloud console, or using the Cloud shell.  \n\n**Using the Google Cloud console** \n\n1. Log in to your Google Cloud console and navigate to Compute Instances (or click [here](https://console.cloud.google.com/compute/instances)). \n2. For each VM on which you want to enable the feature, follow these steps: \n\n   1. Click on the VM name to go to its details page.\n   2. On the upper right area of the page, select EDIT. \n   3. Scroll down to “Security and access” and check boxes accordingly to your desired settings. Please note that you cannot select “Integrity Monitoring” without checking “Turn on vTPM”, as Integrity monitoring is a feature provided by vTPM. \n\n**Using the Cloud shell** \n\n1. Stop the instance: \n\n`gcloud compute instances stop <instanceName>`\n\n2. Turn on vTPM and Integrity Monitoring: \n\n`gcloud compute instances update <instanceName> --shielded-vtpm --shieldedvm-integrity-monitoring`\n\n3. To also turn on Secure Boot (if you have no custom or unsigned drivers on the instance), add the *\\--shielded-vm-secure-boot* parameter to the previous command or execute the following command separately: \n\n`gcloud compute instances update <instanceName> --shielded-vm-secure-boot` \n\n4. After applying the changes, restart the instance: \n\n`gcloud compute instances start <instanceName>`\n\nNote that Shielded VM is not enabled by default on a VM. However, you can set an organizational policy that will cause all the future VMs to have Shielded VM enabled. To do that, click [here](https://console.cloud.google.com/iam-admin/orgpolicies/compute-requireShieldedVm).  \n\n\n\n#### Other configurations that you should consider for your Google Cloud VMs \n\nThere are some settings you should always check when assessing the security of a virtual machine. Here are just a few examples: \n\n* Use **confidential computing** to [encrypt in-use data](https://cyscale.com/blog/types-of-encryption/). In this way, your data that is processed, trained on, used, queried on your VMs remains encrypted and is not at risk. \n* **Close management ports** if not needed: RDP (3389), SSH (21), WINRM (5985), these are all open gateways for hackers to intrude in your VMs and further down your cloud environment. \n* **Apply patches, OS updates** and any other available updates on your VMs to keep them up to date with the current versions of software and ensure that you’re not leaving them vulnerable to found exploits. \n* **Do not use public IP addresses** for your Compute instances, if possible. Public IP addresses introduce the same vulnerability in your environment as management ports: you increase your attack surface and expose the VM to the Internet. Instead, use load balancers to “hide” your VMs. \n\nAlthough there are many aspects you should consider when setting up your cloud [infrastructure](https://cyscale.com/blog/cloud-infrastructure-security/) securely, use this article as a checklist to tick off some of the most important security configurations."}}],"blueBird":{"data":{"blueBird":{"childImageSharp":{"gatsbyImageData":{"layout":"fixed","images":{"fallback":{"src":"/static/a7e41d4813d3dfc105b466b26564c454/42463/cyscale-blue-bird.png","srcSet":"/static/a7e41d4813d3dfc105b466b26564c454/42463/cyscale-blue-bird.png 386w,\n/static/a7e41d4813d3dfc105b466b26564c454/74bb1/cyscale-blue-bird.png 772w","sizes":"386px"},"sources":[{"srcSet":"/static/a7e41d4813d3dfc105b466b26564c454/e03ca/cyscale-blue-bird.webp 386w,\n/static/a7e41d4813d3dfc105b466b26564c454/e7f31/cyscale-blue-bird.webp 772w","type":"image/webp","sizes":"386px"}]},"width":386,"height":351}}}}},"compliceToolbox":{"data":{"blueBird":{"childImageSharp":{"gatsbyImageData":{"layout":"fixed","images":{"fallback":{"src":"/static/a005e76c44581c38cf1d4d1e54e522c0/91b0f/compliance-toolbox-blog.png","srcSet":"/static/a005e76c44581c38cf1d4d1e54e522c0/91b0f/compliance-toolbox-blog.png 366w,\n/static/a005e76c44581c38cf1d4d1e54e522c0/d39b6/compliance-toolbox-blog.png 732w","sizes":"366px"},"sources":[{"srcSet":"/static/a005e76c44581c38cf1d4d1e54e522c0/7257f/compliance-toolbox-blog.webp 366w,\n/static/a005e76c44581c38cf1d4d1e54e522c0/a67bf/compliance-toolbox-blog.webp 732w","type":"image/webp","sizes":"366px"}]},"width":366,"height":333}}}}}}},"staticQueryHashes":["3058837307","3765828210","4109069157","632500807","981947644"],"slicesMap":{}}